{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32aeda1-2938-4bdc-8a82-6e9fdea53e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.5.1)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.1.0-py3-none-any.whl (503 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multiprocess, multidict, hf-xet, frozenlist, aiohappyeyeballs, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: pyarrow\n",
      "\u001b[2K    Found existing installation: pyarrow 20.0.0\n",
      "\u001b[2K    Uninstalling pyarrow-20.0.0:\n",
      "\u001b[2K      Successfully uninstalled pyarrow-20.0.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/13\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [datasets]/13\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate>=1.4.0 (from trl)\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from trl) (4.1.0)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.35.0)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=1.4.0->trl)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.56.1->trl)\n",
      "  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, accelerate, transformers, trl\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [trl]\u001b[32m5/6\u001b[0m [trl]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 regex-2025.9.1 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1 trl-0.23.0\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bbbcb5-95bf-43cb-a713-c12344357ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoChatGPT'...\n",
      "remote: Enumerating objects: 304, done.\u001b[K\n",
      "remote: Total 304 (delta 0), reused 0 (delta 0), pack-reused 304 (from 1)\u001b[K\n",
      "Receiving objects: 100% (304/304), 57.72 MiB | 19.42 MiB/s, done.\n",
      "Resolving deltas: 100% (123/123), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce1f40e-e7d4-4cf7-ba61-caf0834e310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 수정 완료: chatgpt/trainer/callbacks/save_checkpoint.py\n",
      "✅ 수정 완료: chatgpt/trainer/strategies/__init__.py\n",
      "✅ 수정 완료: chatgpt/dataset/reward_dataset.py\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 파일의 8번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: class RewardDataset(Dataset):\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 수정할 내용이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94eead3d-c361-4f03-9eaa-f6478f775fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:2.7.1+cu118\n",
      "Cuda version: 11.8\n",
      "transformers version: 4.56.1\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "# 만일 아래 모듈이 불러와지지 않는다면 Clone 및 수정을 잘 진행했는지 확인해주세요.\n",
    "from chatgpt.trainer.strategies import NaiveStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a18b0eb5-d1b8-433c-bb67-8d50563b7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95a564dc-24fe-4723-b6ad-38432d70abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e2b3bac-8c6c-4122-86f9-b98c4722ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a615892-e5d2-4c89-ac6c-1b114683a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e6a889a-fefb-44ec-9479-a544b1443411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7880c0d4-c4dc-49a0-abd2-ef024b1670b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72e07f18-9f1a-4757-80cd-e10b50185758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8d535d0-fe90-4f3e-acab-f4ac5bebd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    report_to=\"none\",\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4063226b-e4ee-46c2-8f79-c214dfc0f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.684700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('models/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25ae704d-ef50-4e69-ac1b-770a2d553899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 인공지능 언어모델로써 답변을 생성하는 AI 어시스턴트이기 때문에 정확한 답변을 제공할 수 없습니다. 하지만 일반적으로 불고기용 고기는 쇠고기의 종류에 따라 다양한 종류가 있을 수 있습니다. 예를 들어, 소고기, 돼지고기, 닭고기 등 다양한 종류의 불고기용\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행했습니다.son은 47대 부통령직을 역임하였습니다.son이 46대 부통령직을 맡은 년도는 1946년입니다.son은 1949년에 부통령직을 맡았습니다.son의 부통령직 수행년도는 1952년입니다.son과 그의 부통령직 수행년에 대한 자세한 정보는\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다. 하지만 시카고는 미국 캘리포니아주 로스앤젤레스에 위치한 도시입니다. 시카고는 미국에서 가장 큰 도시 중 하나이며, 많은 사람들이 살고 있는 지역 중 하나입니다. 따라서 시카고는 미국 내에서 가장 인기 있는 도시 중 하나입니다.子, \"시카고\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 미세먼지에 대한 정보를 알 수 없습니다. 하지만 미세먼지 예보 앱을 통해 미세먼지 농도를 확인해보시는 것을 추천드립니다. 또한 미세먼지 농도가 높은 날에는 대중교통을 이용하시는 것도 좋은 방법입니다. 만약 미세먼지가 심한\n"
     ]
    }
   ],
   "source": [
    "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0888a9b-f097-452d-8446-484d1e858a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb57c10c-8342-4dd2-844c-fd21d06e7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b09fe40-3df3-4d3d-a691-8af92ce56ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from chatgpt.trainer.rm import RewardModelTrainer\n",
    "\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1a9abec-317c-4a2b-8644-db50e1791fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5d9f784-0d02-46bf-9430-7400b5bceba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9807ded9-b871-4dad-be10-6845ed5ee2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f92b8ca4-c8a0-4b15-a435-3dbaddd52668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "541f8770-5710-4bb9-b6e7-e7a4d9113ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e288b0a92348ffa848282f0bdcc982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da90265efd61437b8f2a973ccd3c8517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ba78577-317b-4f8e-82fd-c302ad426f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=torch.optim.Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bd48bf0-3c7d-46dd-aad4-aa114fd9c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<02:45,  1.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<02:45,  1.50it/s, loss=0.725]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:00,  1.37it/s, loss=0.725]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:00,  1.37it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:18,  1.24it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:18,  1.24it/s, loss=0.56] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:26,  1.19it/s, loss=0.56]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:26,  1.19it/s, loss=0.534]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:30,  1.16it/s, loss=0.534]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:30,  1.16it/s, loss=0.446]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:33,  1.14it/s, loss=0.446]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:33,  1.14it/s, loss=0.357]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:05<03:35,  1.13it/s, loss=0.357]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:35,  1.13it/s, loss=0.557]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:35,  1.12it/s, loss=0.557]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:35,  1.12it/s, loss=2.33] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:35,  1.12it/s, loss=2.33]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:35,  1.12it/s, loss=0.292]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:36,  1.11it/s, loss=0.292]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:36,  1.11it/s, loss=0.51] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:36,  1.11it/s, loss=0.51]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:36,  1.11it/s, loss=0.32]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:35,  1.10it/s, loss=0.32]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:35,  1.10it/s, loss=0.455]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:35,  1.10it/s, loss=0.455]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:35,  1.10it/s, loss=0.104]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:34,  1.10it/s, loss=0.104]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:34,  1.10it/s, loss=1.18] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:34,  1.10it/s, loss=1.18]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:34,  1.10it/s, loss=0.543]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:33,  1.10it/s, loss=0.543]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:33,  1.10it/s, loss=1.07] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:32,  1.10it/s, loss=1.07]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:32,  1.10it/s, loss=0.715]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:31,  1.10it/s, loss=0.715]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:31,  1.10it/s, loss=0.472]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:30,  1.10it/s, loss=0.472]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:30,  1.10it/s, loss=0.772]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:29,  1.10it/s, loss=0.772]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:29,  1.10it/s, loss=0.482]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:28,  1.10it/s, loss=0.482]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:19<03:28,  1.10it/s, loss=0.762]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:27,  1.10it/s, loss=0.762]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:20<03:27,  1.10it/s, loss=0.672]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:26,  1.10it/s, loss=0.672]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:26,  1.10it/s, loss=0.834]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:25,  1.10it/s, loss=0.834]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:25,  1.10it/s, loss=0.649]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:24,  1.10it/s, loss=0.649]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:24,  1.10it/s, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:22,  1.11it/s, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:22,  1.11it/s, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:20,  1.11it/s, loss=0.741]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:20,  1.11it/s, loss=0.628]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:19,  1.11it/s, loss=0.628]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:19,  1.11it/s, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:17,  1.12it/s, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:17,  1.12it/s, loss=0.538]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:16,  1.12it/s, loss=0.538]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:16,  1.12it/s, loss=0.538]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:15,  1.12it/s, loss=0.538]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:28<03:15,  1.12it/s, loss=0.448]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:13,  1.12it/s, loss=0.448]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:13,  1.12it/s, loss=0.77] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:12,  1.13it/s, loss=0.77]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:12,  1.13it/s, loss=0.468]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:30<03:11,  1.13it/s, loss=0.468]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:30<03:11,  1.13it/s, loss=0.509]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:31<03:10,  1.13it/s, loss=0.509]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:31<03:10,  1.13it/s, loss=0.803]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:32<03:08,  1.13it/s, loss=0.803]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:32<03:08,  1.13it/s, loss=1.25] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:07,  1.13it/s, loss=1.25]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:07,  1.13it/s, loss=0.719]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:06,  1.14it/s, loss=0.719]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:06,  1.14it/s, loss=0.666]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:05,  1.14it/s, loss=0.666]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:05,  1.14it/s, loss=0.912]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:03,  1.14it/s, loss=0.912]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:03,  1.14it/s, loss=0.537]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:36<03:02,  1.15it/s, loss=0.537]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:36<03:02,  1.15it/s, loss=0.446]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:37<03:01,  1.15it/s, loss=0.446]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:37<03:01,  1.15it/s, loss=0.641]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:38<03:00,  1.15it/s, loss=0.641]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:38<03:00,  1.15it/s, loss=0.664]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:39<02:59,  1.15it/s, loss=0.664]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:39<02:59,  1.15it/s, loss=0.866]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:58,  1.15it/s, loss=0.866]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:40<02:58,  1.15it/s, loss=0.547]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:57,  1.15it/s, loss=0.547]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:41<02:57,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<02:55,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:42<02:55,  1.15it/s, loss=0.656]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<02:54,  1.15it/s, loss=0.656]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<02:54,  1.15it/s, loss=0.698]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<02:53,  1.16it/s, loss=0.698]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<02:53,  1.16it/s, loss=0.597]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:44<02:52,  1.16it/s, loss=0.597]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:44<02:52,  1.16it/s, loss=0.844]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:45<02:51,  1.16it/s, loss=0.844]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:45<02:51,  1.16it/s, loss=0.443]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:50,  1.16it/s, loss=0.443]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:46<02:50,  1.16it/s, loss=0.66] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:49,  1.16it/s, loss=0.66]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:47<02:49,  1.16it/s, loss=0.465]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:49,  1.16it/s, loss=0.465]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:48<02:49,  1.16it/s, loss=0.366]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:47,  1.16it/s, loss=0.366]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:47,  1.16it/s, loss=0.551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:47,  1.16it/s, loss=0.551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:47,  1.16it/s, loss=0.586]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:45,  1.16it/s, loss=0.586]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:45,  1.16it/s, loss=0.515]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:44,  1.17it/s, loss=0.515]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:44,  1.17it/s, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:44,  1.16it/s, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:52<02:44,  1.16it/s, loss=0.564]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:43,  1.16it/s, loss=0.564]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:53<02:43,  1.16it/s, loss=0.277]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:41,  1.17it/s, loss=0.277]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:54<02:41,  1.17it/s, loss=0.964]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:40,  1.17it/s, loss=0.964]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:40,  1.17it/s, loss=0.184]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:40,  1.17it/s, loss=0.184]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:40,  1.17it/s, loss=1.1]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:39,  1.17it/s, loss=1.1]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:39,  1.17it/s, loss=0.303]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:38,  1.17it/s, loss=0.303]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:38,  1.17it/s, loss=0.766]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:37,  1.17it/s, loss=0.766]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:58<02:37,  1.17it/s, loss=0.116]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:36,  1.17it/s, loss=0.116]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:59<02:36,  1.17it/s, loss=0.964]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:36,  1.17it/s, loss=0.964]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:00<02:36,  1.17it/s, loss=0.313]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:35,  1.17it/s, loss=0.313]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:35,  1.17it/s, loss=0.768]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:34,  1.17it/s, loss=0.768]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:34,  1.17it/s, loss=0.758]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:33,  1.17it/s, loss=0.758]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:33,  1.17it/s, loss=0.52] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:03<02:33,  1.16it/s, loss=0.52]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:03<02:33,  1.16it/s, loss=0.843]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:32,  1.16it/s, loss=0.843]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:04<02:32,  1.16it/s, loss=0.73] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:31,  1.16it/s, loss=0.73]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:05<02:31,  1.16it/s, loss=0.548]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:30,  1.16it/s, loss=0.548]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:06<02:30,  1.16it/s, loss=0.623]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.623]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:29,  1.16it/s, loss=0.725]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:07<02:28,  1.16it/s, loss=0.725]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:07<02:28,  1.16it/s, loss=0.474]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:08<02:28,  1.16it/s, loss=0.474]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:08<02:28,  1.16it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:09<02:27,  1.16it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:09<02:27,  1.16it/s, loss=0.592]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:26,  1.16it/s, loss=0.592]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:10<02:26,  1.16it/s, loss=0.7]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:25,  1.16it/s, loss=0.7]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:11<02:25,  1.16it/s, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:24,  1.16it/s, loss=0.444]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:12<02:24,  1.16it/s, loss=0.8]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:24,  1.16it/s, loss=0.8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:13<02:24,  1.16it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:23,  1.16it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:23,  1.16it/s, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:22,  1.16it/s, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:22,  1.16it/s, loss=0.683]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:15<02:21,  1.16it/s, loss=0.683]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:15<02:21,  1.16it/s, loss=0.556]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:16<02:20,  1.16it/s, loss=0.556]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:16<02:20,  1.16it/s, loss=0.542]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:20,  1.15it/s, loss=0.542]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:17<02:20,  1.15it/s, loss=0.857]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:19,  1.15it/s, loss=0.857]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:18<02:19,  1.15it/s, loss=0.834]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:18,  1.16it/s, loss=0.834]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:19<02:18,  1.16it/s, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:17,  1.15it/s, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:17,  1.15it/s, loss=0.68] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:17,  1.15it/s, loss=0.68]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:17,  1.15it/s, loss=0.635]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:16,  1.15it/s, loss=0.635]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:16,  1.15it/s, loss=0.8]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:22<02:15,  1.15it/s, loss=0.8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:22<02:15,  1.15it/s, loss=0.716]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:23<02:14,  1.15it/s, loss=0.716]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:23<02:14,  1.15it/s, loss=0.535]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:14,  1.15it/s, loss=0.535]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:24<02:14,  1.15it/s, loss=0.711]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:13,  1.15it/s, loss=0.711]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:25<02:13,  1.15it/s, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:12,  1.15it/s, loss=0.799]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:26<02:12,  1.15it/s, loss=0.78] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:11,  1.15it/s, loss=0.78]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:11,  1.15it/s, loss=0.605]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:11,  1.14it/s, loss=0.605]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:11,  1.14it/s, loss=0.654]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:09,  1.15it/s, loss=0.654]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:09,  1.15it/s, loss=0.684]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:09,  1.15it/s, loss=0.684]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:09,  1.15it/s, loss=0.678]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:08,  1.15it/s, loss=0.678]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:30<02:08,  1.15it/s, loss=0.693]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:07,  1.15it/s, loss=0.693]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:31<02:07,  1.15it/s, loss=0.86] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:06,  1.15it/s, loss=0.86]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:32<02:06,  1.15it/s, loss=0.631]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:06,  1.14it/s, loss=0.631]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:33<02:06,  1.14it/s, loss=0.617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:05,  1.14it/s, loss=0.617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:05,  1.14it/s, loss=0.743]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:04,  1.14it/s, loss=0.743]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:04,  1.14it/s, loss=0.577]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:03,  1.14it/s, loss=0.577]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:03,  1.14it/s, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:02,  1.14it/s, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:02,  1.14it/s, loss=0.716]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:37<02:01,  1.14it/s, loss=0.716]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:37<02:01,  1.14it/s, loss=0.588]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<02:01,  1.14it/s, loss=0.588]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:38<02:01,  1.14it/s, loss=0.624]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<02:00,  1.14it/s, loss=0.624]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:39<02:00,  1.14it/s, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:39<01:59,  1.14it/s, loss=0.663]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:40<01:59,  1.14it/s, loss=0.838]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:58,  1.14it/s, loss=0.838]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:58,  1.14it/s, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:57,  1.14it/s, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:57,  1.14it/s, loss=0.568]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:56,  1.14it/s, loss=0.568]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:56,  1.14it/s, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:55,  1.14it/s, loss=0.516]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:55,  1.14it/s, loss=0.531]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:44<01:55,  1.14it/s, loss=0.531]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:44<01:55,  1.14it/s, loss=0.473]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:54,  1.13it/s, loss=0.473]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:45<01:54,  1.13it/s, loss=1.13] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:53,  1.14it/s, loss=1.13]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:46<01:53,  1.14it/s, loss=0.784]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:46<01:52,  1.14it/s, loss=0.784]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:47<01:52,  1.14it/s, loss=0.66] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:51,  1.14it/s, loss=0.66]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:51,  1.14it/s, loss=0.54]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:50,  1.14it/s, loss=0.54]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:50,  1.14it/s, loss=0.713]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:49,  1.14it/s, loss=0.713]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:49,  1.14it/s, loss=0.647]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:50<01:49,  1.14it/s, loss=0.647]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:50<01:49,  1.14it/s, loss=0.653]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:51<01:48,  1.14it/s, loss=0.653]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:51<01:48,  1.14it/s, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:47,  1.14it/s, loss=0.584]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:52<01:47,  1.14it/s, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:46,  1.14it/s, loss=0.636]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:53<01:46,  1.14it/s, loss=0.617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:53<01:45,  1.14it/s, loss=0.617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:54<01:45,  1.14it/s, loss=0.583]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:54<01:44,  1.14it/s, loss=0.583]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:55<01:44,  1.14it/s, loss=0.994]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:43,  1.14it/s, loss=0.994]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:43,  1.14it/s, loss=0.599]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:42,  1.14it/s, loss=0.599]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:42,  1.14it/s, loss=0.812]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:57<01:41,  1.14it/s, loss=0.812]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:57<01:41,  1.14it/s, loss=0.777]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:58<01:40,  1.14it/s, loss=0.777]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:58<01:40,  1.14it/s, loss=0.647]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:58<01:39,  1.14it/s, loss=0.647]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:59<01:39,  1.14it/s, loss=0.696]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:59<01:39,  1.14it/s, loss=0.696]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:00<01:39,  1.14it/s, loss=0.5]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:00<01:38,  1.14it/s, loss=0.5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:01<01:38,  1.14it/s, loss=0.61]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:01<01:37,  1.14it/s, loss=0.61]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:02<01:37,  1.14it/s, loss=0.514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:36,  1.14it/s, loss=0.514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:36,  1.14it/s, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:35,  1.14it/s, loss=0.499]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:35,  1.14it/s, loss=0.631]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:04<01:34,  1.14it/s, loss=0.631]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:04<01:34,  1.14it/s, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:05<01:33,  1.14it/s, loss=0.829]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:05<01:33,  1.14it/s, loss=0.556]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:05<01:32,  1.14it/s, loss=0.556]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:06<01:32,  1.14it/s, loss=0.501]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:06<01:31,  1.15it/s, loss=0.501]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:07<01:31,  1.15it/s, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:07<01:30,  1.15it/s, loss=0.634]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:08<01:30,  1.15it/s, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:08<01:29,  1.15it/s, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:09<01:29,  1.15it/s, loss=0.336]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:28,  1.15it/s, loss=0.336]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:28,  1.15it/s, loss=0.638]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:28,  1.15it/s, loss=0.638]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:28,  1.15it/s, loss=0.688]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:11<01:27,  1.15it/s, loss=0.688]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:11<01:27,  1.15it/s, loss=0.581]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:12<01:26,  1.15it/s, loss=0.581]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:12<01:26,  1.15it/s, loss=0.28] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:12<01:25,  1.15it/s, loss=0.28]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:13<01:25,  1.15it/s, loss=0.586]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:13<01:24,  1.15it/s, loss=0.586]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:14<01:24,  1.15it/s, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:14<01:23,  1.15it/s, loss=0.667]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:15<01:23,  1.15it/s, loss=1.16] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:15<01:22,  1.15it/s, loss=1.16]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:15<01:22,  1.15it/s, loss=0.917]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:21,  1.15it/s, loss=0.917]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:21,  1.15it/s, loss=0.467]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:21,  1.15it/s, loss=0.467]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:21,  1.15it/s, loss=0.475]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:18<01:20,  1.15it/s, loss=0.475]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:18<01:20,  1.15it/s, loss=0.71] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:19<01:19,  1.15it/s, loss=0.71]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:19<01:19,  1.15it/s, loss=0.713]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:19<01:18,  1.15it/s, loss=0.713]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:20<01:18,  1.15it/s, loss=0.399]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:20<01:17,  1.15it/s, loss=0.399]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:21<01:17,  1.15it/s, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:21<01:16,  1.15it/s, loss=0.669]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:22<01:16,  1.15it/s, loss=0.464]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:22<01:15,  1.15it/s, loss=0.464]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:22<01:15,  1.15it/s, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:23<01:15,  1.15it/s, loss=0.365]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:23<01:15,  1.15it/s, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:14,  1.15it/s, loss=0.598]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:14,  1.15it/s, loss=0.972]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:25<01:13,  1.15it/s, loss=0.972]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:25<01:13,  1.15it/s, loss=0.319]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:25<01:12,  1.15it/s, loss=0.319]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:26<01:12,  1.15it/s, loss=0.387]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:26<01:11,  1.15it/s, loss=0.387]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:27<01:11,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:27<01:10,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:28<01:10,  1.15it/s, loss=0.437]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:28<01:09,  1.15it/s, loss=0.437]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:29<01:09,  1.15it/s, loss=1.02] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:29<01:08,  1.15it/s, loss=1.02]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:29<01:08,  1.15it/s, loss=0.721]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:30<01:07,  1.15it/s, loss=0.721]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:30<01:07,  1.15it/s, loss=0.955]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:31<01:06,  1.15it/s, loss=0.955]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:31<01:06,  1.15it/s, loss=0.49] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:32<01:06,  1.15it/s, loss=0.49]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:32<01:06,  1.15it/s, loss=0.339]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:32<01:05,  1.15it/s, loss=0.339]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:33<01:05,  1.15it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:33<01:04,  1.15it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:34<01:04,  1.15it/s, loss=0.476]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:34<01:03,  1.15it/s, loss=0.476]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:35<01:03,  1.15it/s, loss=0.41] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:35<01:02,  1.15it/s, loss=0.41]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:35<01:02,  1.15it/s, loss=0.378]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:36<01:01,  1.15it/s, loss=0.378]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:36<01:01,  1.15it/s, loss=0.51] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:37<01:00,  1.15it/s, loss=0.51]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:37<01:00,  1.15it/s, loss=0.651]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:38<01:00,  1.15it/s, loss=0.651]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:38<01:00,  1.15it/s, loss=0.323]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:39<00:59,  1.15it/s, loss=0.323]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:39<00:59,  1.15it/s, loss=0.907]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:39<00:58,  1.15it/s, loss=0.907]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:40<00:58,  1.15it/s, loss=1.17] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:40<00:57,  1.15it/s, loss=1.17]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:41<00:57,  1.15it/s, loss=0.826]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:41<00:56,  1.15it/s, loss=0.826]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:42<00:56,  1.15it/s, loss=0.537]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:42<00:55,  1.15it/s, loss=0.537]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:42<00:55,  1.15it/s, loss=0.383]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:43<00:54,  1.15it/s, loss=0.383]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:43<00:54,  1.15it/s, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:44<00:53,  1.15it/s, loss=0.351]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:44<00:53,  1.15it/s, loss=0.596]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:45<00:52,  1.15it/s, loss=0.596]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:45<00:52,  1.15it/s, loss=0.851]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:45<00:52,  1.15it/s, loss=0.851]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:46<00:52,  1.15it/s, loss=0.393]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:46<00:51,  1.15it/s, loss=0.393]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:47<00:51,  1.15it/s, loss=0.252]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:47<00:50,  1.15it/s, loss=0.252]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:48<00:50,  1.15it/s, loss=0.505]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:48<00:49,  1.15it/s, loss=0.505]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:49<00:49,  1.15it/s, loss=0.803]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:49<00:48,  1.15it/s, loss=0.803]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:49<00:48,  1.15it/s, loss=0.668]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:50<00:47,  1.15it/s, loss=0.668]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:50<00:47,  1.15it/s, loss=0.344]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:51<00:47,  1.15it/s, loss=0.344]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:51<00:47,  1.15it/s, loss=0.993]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:52<00:46,  1.15it/s, loss=0.993]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:52<00:46,  1.15it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:52<00:45,  1.15it/s, loss=0.632]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:53<00:45,  1.15it/s, loss=0.579]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:53<00:44,  1.15it/s, loss=0.579]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:54<00:44,  1.15it/s, loss=0.416]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:54<00:43,  1.15it/s, loss=0.416]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:55<00:43,  1.15it/s, loss=0.404]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:55<00:42,  1.15it/s, loss=0.404]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:56<00:42,  1.15it/s, loss=0.512]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:56<00:41,  1.15it/s, loss=0.512]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:56<00:41,  1.15it/s, loss=0.892]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:57<00:40,  1.15it/s, loss=0.892]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:57<00:40,  1.15it/s, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:58<00:40,  1.15it/s, loss=0.644]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:58<00:40,  1.15it/s, loss=0.502]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:59<00:39,  1.15it/s, loss=0.502]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:59<00:39,  1.15it/s, loss=0.554]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:59<00:38,  1.15it/s, loss=0.554]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:00<00:38,  1.15it/s, loss=0.757]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:00<00:37,  1.15it/s, loss=0.757]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:01<00:37,  1.15it/s, loss=0.514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:01<00:36,  1.15it/s, loss=0.514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:02<00:36,  1.15it/s, loss=0.708]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:02<00:35,  1.15it/s, loss=0.708]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:02<00:35,  1.15it/s, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:03<00:34,  1.15it/s, loss=0.646]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:03<00:34,  1.15it/s, loss=0.637]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:04<00:33,  1.15it/s, loss=0.637]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:04<00:33,  1.15it/s, loss=0.407]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:05<00:33,  1.15it/s, loss=0.407]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:05<00:33,  1.15it/s, loss=0.552]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:06<00:32,  1.15it/s, loss=0.552]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:06<00:32,  1.15it/s, loss=0.41] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:06<00:31,  1.15it/s, loss=0.41]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:07<00:31,  1.15it/s, loss=1.16]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:07<00:30,  1.15it/s, loss=1.16]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:08<00:30,  1.15it/s, loss=0.456]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:08<00:29,  1.15it/s, loss=0.456]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:09<00:29,  1.15it/s, loss=0.835]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:09<00:28,  1.15it/s, loss=0.835]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:09<00:28,  1.15it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:10<00:27,  1.15it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:10<00:27,  1.15it/s, loss=0.571]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:11<00:27,  1.15it/s, loss=0.571]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:11<00:27,  1.15it/s, loss=0.385]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:12<00:26,  1.15it/s, loss=0.385]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:12<00:26,  1.15it/s, loss=0.676]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:12<00:25,  1.15it/s, loss=0.676]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:13<00:25,  1.15it/s, loss=0.59] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:13<00:24,  1.15it/s, loss=0.59]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:14<00:24,  1.15it/s, loss=0.35]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:14<00:23,  1.15it/s, loss=0.35]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:15<00:23,  1.15it/s, loss=0.754]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:15<00:22,  1.15it/s, loss=0.754]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:16<00:22,  1.15it/s, loss=0.74] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:16<00:21,  1.15it/s, loss=0.74]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:16<00:21,  1.15it/s, loss=0.563]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:17<00:20,  1.15it/s, loss=0.563]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:17<00:20,  1.15it/s, loss=0.658]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:18<00:20,  1.15it/s, loss=0.658]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:18<00:20,  1.15it/s, loss=0.715]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:19<00:19,  1.15it/s, loss=0.715]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:19<00:19,  1.15it/s, loss=0.559]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:19<00:18,  1.15it/s, loss=0.559]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:20<00:18,  1.15it/s, loss=0.743]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:20<00:17,  1.15it/s, loss=0.743]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:21<00:17,  1.15it/s, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:21<00:16,  1.15it/s, loss=0.675]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:22<00:16,  1.15it/s, loss=0.439]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:22<00:15,  1.15it/s, loss=0.439]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:23<00:15,  1.15it/s, loss=0.8]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:23<00:14,  1.15it/s, loss=0.8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:23<00:14,  1.15it/s, loss=0.481]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:24<00:13,  1.15it/s, loss=0.481]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:24<00:13,  1.15it/s, loss=0.49] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:25<00:13,  1.15it/s, loss=0.49]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:25<00:13,  1.15it/s, loss=0.966]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:26<00:12,  1.15it/s, loss=0.966]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:26<00:12,  1.15it/s, loss=0.565]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:26<00:11,  1.15it/s, loss=0.565]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:27<00:11,  1.15it/s, loss=0.54] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:27<00:10,  1.15it/s, loss=0.54]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:28<00:10,  1.15it/s, loss=0.688]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:28<00:09,  1.15it/s, loss=0.688]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:29<00:09,  1.15it/s, loss=0.694]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:29<00:08,  1.15it/s, loss=0.694]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:29<00:08,  1.15it/s, loss=0.614]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:30<00:07,  1.15it/s, loss=0.614]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:30<00:07,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:31<00:06,  1.15it/s, loss=0.491]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:31<00:06,  1.15it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:32<00:06,  1.15it/s, loss=0.881]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:32<00:06,  1.15it/s, loss=0.547]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:33<00:05,  1.15it/s, loss=0.547]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:33<00:05,  1.15it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:33<00:04,  1.15it/s, loss=0.616]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:34<00:04,  1.15it/s, loss=0.83] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:34<00:03,  1.15it/s, loss=0.83]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:35<00:03,  1.15it/s, loss=0.886]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:35<00:02,  1.15it/s, loss=0.886]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:36<00:02,  1.15it/s, loss=0.362]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:36<00:01,  1.15it/s, loss=0.362]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:36<00:01,  1.15it/s, loss=1.22] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:37<00:00,  1.14it/s, loss=1.22]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:37<00:00,  1.14it/s, loss=0.708]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:38<00:00,  1.14it/s, loss=0.708]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:38<00:00,  1.14it/s, loss=0.502]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:51<00:00,  1.08it/s, loss=0.624, dist_mean=0.235]\u001b[A\u001b[A\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.87s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afd9ebed-dbfb-42ab-b6ea-1d355c07d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -0.7\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da79b74d-c5e5-4d8d-a8e8-39185631706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0820d83a-068e-4ee8-99b9-d3fc9d305d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_candidates = [ \n",
    "    \"서울 어린이대공원 동물원이나 놀이시설도 추천해요. 입장료가 저렴해서 가족 단위 방문객이 많습니다.\",\n",
    "    \"날씨가 좋으면 잠실 롯데월드타워 전망대, 실내활동이면 국립중앙박물관 어린이박물관도 좋아요.\",\n",
    "    \"아이 연령대에 따라 다르지만, 5~7세라면 코엑스 키자니아 같은 체험형 공간이 적합합니다.\",\n",
    "\n",
    "    \"서울에는 다양한 체험 공간이 있어요. 인터넷 검색해보시면 많은 정보를 얻을 수 있습니다.\",\n",
    "    \"추천드릴 수는 없지만 서울에는 어린이들을 위한 좋은 장소가 많습니다.\",\n",
    "    \"아이와 함께라면 실내 전시관이나 놀이시설이 무난합니다.\",\n",
    "\n",
    "    \"저는 인공지능이기 때문에 추천이 불가능합니다.\",\n",
    "    \"알려드릴 수 없습니다.\",\n",
    "    \"모릅니다.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77562d43-dcd5-4aa4-a4a1-8c4513c2591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 서울 어린이대공원 동물원이나 놀이시설도 추천해요. 입장료가 저렴해서 가족 단위 방문객이 많습니다.\n",
      "reward score: -0.6\n",
      "input: 날씨가 좋으면 잠실 롯데월드타워 전망대, 실내활동이면 국립중앙박물관 어린이박물관도 좋아요.\n",
      "reward score: -0.9\n",
      "input: 아이 연령대에 따라 다르지만, 5~7세라면 코엑스 키자니아 같은 체험형 공간이 적합합니다.\n",
      "reward score: -0.7\n",
      "input: 서울에는 다양한 체험 공간이 있어요. 인터넷 검색해보시면 많은 정보를 얻을 수 있습니다.\n",
      "reward score: -0.9\n",
      "input: 추천드릴 수는 없지만 서울에는 어린이들을 위한 좋은 장소가 많습니다.\n",
      "reward score: -0.8\n",
      "input: 아이와 함께라면 실내 전시관이나 놀이시설이 무난합니다.\n",
      "reward score: -1.0\n",
      "input: 저는 인공지능이기 때문에 추천이 불가능합니다.\n",
      "reward score: -0.9\n",
      "input: 알려드릴 수 없습니다.\n",
      "reward score: -1.1\n",
      "input: 모릅니다.\n",
      "reward score: -3.4\n"
     ]
    }
   ],
   "source": [
    "for i in my_candidates:\n",
    "    inference_RM(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bace9dd1-4a0c-4b8f-b79c-e5dab5055d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_candidates = [\n",
    "    \"규칙적인 운동과 충분한 수면이 가장 중요합니다. 하루 30분 정도 빠르게 걷기만 해도 건강에 큰 도움이 됩니다.\",\n",
    "    \"채소와 과일을 자주 섭취하고, 가공식품과 설탕 섭취를 줄이세요. 또한 하루에 물을 1.5리터 이상 마시는 것이 좋습니다.\",\n",
    "    \"스트레칭은 간단하지만 효과적이에요. 아침에 5분만 해도 혈액순환과 집중력 향상에 도움이 됩니다.\",\n",
    "\n",
    "\n",
    "    \"건강을 지키려면 꾸준히 관리하는 것이 필요합니다.\",\n",
    "    \"운동도 좋고 식습관도 중요합니다.\",\n",
    "    \"잘 먹고 잘 자는 것이 건강에 좋아요.\",\n",
    "\n",
    "    \"저는 인공지능이기 때문에 건강 관리 방법을 알려드릴 수 없습니다.\",\n",
    "    \"모릅니다.\",\n",
    "    \"추천할 수 없습니다.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e759ac0d-94a5-4b00-8179-902f7a0ac9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 서울 어린이대공원 동물원이나 놀이시설도 추천해요. 입장료가 저렴해서 가족 단위 방문객이 많습니다.\n",
      "reward score: -0.6\n",
      "input: 날씨가 좋으면 잠실 롯데월드타워 전망대, 실내활동이면 국립중앙박물관 어린이박물관도 좋아요.\n",
      "reward score: -0.9\n",
      "input: 아이 연령대에 따라 다르지만, 5~7세라면 코엑스 키자니아 같은 체험형 공간이 적합합니다.\n",
      "reward score: -0.7\n",
      "input: 서울에는 다양한 체험 공간이 있어요. 인터넷 검색해보시면 많은 정보를 얻을 수 있습니다.\n",
      "reward score: -0.9\n",
      "input: 추천드릴 수는 없지만 서울에는 어린이들을 위한 좋은 장소가 많습니다.\n",
      "reward score: -0.8\n",
      "input: 아이와 함께라면 실내 전시관이나 놀이시설이 무난합니다.\n",
      "reward score: -1.0\n",
      "input: 저는 인공지능이기 때문에 추천이 불가능합니다.\n",
      "reward score: -0.9\n",
      "input: 알려드릴 수 없습니다.\n",
      "reward score: -1.1\n",
      "input: 모릅니다.\n",
      "reward score: -3.4\n"
     ]
    }
   ],
   "source": [
    "for i in my_candidates:\n",
    "    inference_RM(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "085cb247-94d8-4d2b-8aec-aa90d4bb869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1985670f-6d33-44ac-b67d-5b39d64bf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='models/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='models/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d647c7aa-5ad3-4bdc-aef4-7351136675fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d52865b1-2a93-4a7b-a559-bd15778ddde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf830173-3cee-4e90-9e02-b60fc492a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62bda702-5d9d-4fcf-aaf9-a713378692de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40cfce34-b9c9-4c78-9f1a-758e3657fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Episode [1/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [1/10]:  33%|███▎      | 1/3 [00:06<00:12,  6.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000796]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0, critic_loss=0.000796]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0, critic_loss=0.0802]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.0802]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.00772]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.00772]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:20<00:00,  6.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [2/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [2/10]:  33%|███▎      | 1/3 [00:06<00:12,  6.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.121, critic_loss=0.0138]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.93it/s, actor_loss=0.121, critic_loss=0.0138]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.93it/s, actor_loss=0.123, critic_loss=0.0475]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.123, critic_loss=0.0475]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.121, critic_loss=0.042] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, actor_loss=0.121, critic_loss=0.042]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [3/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [3/10]:  33%|███▎      | 1/3 [00:05<00:11,  5.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.114, critic_loss=0.0125]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.96it/s, actor_loss=0.114, critic_loss=0.0125]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.96it/s, actor_loss=0.0924, critic_loss=0.000955]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=0.0924, critic_loss=0.000955]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=0.103, critic_loss=0.011]    \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s, actor_loss=0.103, critic_loss=0.011]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [4/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [4/10]:  33%|███▎      | 1/3 [00:05<00:11,  5.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.129, critic_loss=0.023]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s, actor_loss=-0.129, critic_loss=0.023]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.95it/s, actor_loss=-0.162, critic_loss=0.0292]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=-0.162, critic_loss=0.0292]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=-0.171, critic_loss=0.0221]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s, actor_loss=-0.171, critic_loss=0.0221]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [5/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [5/10]:  33%|███▎      | 1/3 [00:05<00:11,  5.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.0653, critic_loss=0.00366]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-0.0653, critic_loss=0.00366]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=-0.059, critic_loss=0.0017]  \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-0.059, critic_loss=0.0017]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-0.0651, critic_loss=0.00894]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-0.0651, critic_loss=0.00894]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [6/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [6/10]:  33%|███▎      | 1/3 [00:06<00:12,  6.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.11, critic_loss=0.0163]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=0.11, critic_loss=0.0163]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=0.13, critic_loss=0.0168]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0.13, critic_loss=0.0168]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0.103, critic_loss=0.00748]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=0.103, critic_loss=0.00748]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:19<00:00,  6.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [7/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Episode [7/10]:  33%|███▎      | 1/3 [00:06<00:12,  6.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:12<00:05,  5.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0263, critic_loss=0.00249]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s, actor_loss=0.0263, critic_loss=0.00249]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.95it/s, actor_loss=0.0248, critic_loss=0.00135]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.0248, critic_loss=0.00135]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.0335, critic_loss=0.00453]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, actor_loss=0.0335, critic_loss=0.00453]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Episode [8/10]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Train epoch:   0%|          | 0/1 [20:42<?, ?it/s]  5.78s/it]\u001b[A\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [20:42<9:14:29, 138.05s/it, loss=0.468]\n",
      "\n",
      "\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s, actor_loss=-0.0899, critic_loss=0.00411]\n",
      "\n",
      "\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\u001b[A\u001b[A\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.29s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-0.0322, critic_loss=0.00154]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.92it/s, actor_loss=-0.0322, critic_loss=0.00154]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.92it/s, actor_loss=-0.0409, critic_loss=0.000853]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s, actor_loss=-0.0409, critic_loss=0.000853]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.93it/s, actor_loss=-0.04, critic_loss=0.0032]    \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, actor_loss=-0.04, critic_loss=0.0032]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:18<00:00,  6.08s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  6.00s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0423, critic_loss=0.00444]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=0.0423, critic_loss=0.00444]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=0.0361, critic_loss=0.00231]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.0361, critic_loss=0.00231]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.0376, critic_loss=0.00139]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=0.0376, critic_loss=0.00139]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt,\n",
    "            num_episodes=10,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "actor.model.save_pretrained('models/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9098cfa3-26f5-4a48-aa04-7ada982af5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'아니다. 일반적으로 고기를 살 때 보통 맵고 부드러운 고기와 고기를 사용하는 것이 권장됩니다. 그러나 고기를 조리하는 방식은 다양하므로 다양한 방법으로 선택할 수 있습니다. 따라서 고기를 조리할 때는 정확한 맛과 식량, 조리 방법에 대해 잘 파악하시고 선택하시는 것이 좋습니다.哉), 불고기용 고기 한우는 주로 고기, 쇠고기, 양고기, 불고기, 닭볶음탕 등이 제공됩니다.哉), 불고기용 고기를 사용하는 이유는 일반적인 상황에서 사용되는 고기 고기보다 더 부드럽고 부드러운 맛과 함께 고기가 제공된다는 장점 때문입니다.哉), 불고기용 고기 한우는 매운맛과 매운맛을 동시에 지니고 있어 매운 맛을 강조할 수 있고, 다양한 음식 재료로 활용할 수 있다는 장점이 있습니다.哉), 불고기용 고기 한우는 일반적으로 소스, 국산, 불고기 등으로 제공됩니다.哉), 불고기용 한우는 주로 소스, 국산, 소스, 혼합, 혼합, 육류, 고기, 불고기용으로 제공됩니다.哉)는 고기를 조리할 때 사용되는 재료와 종류의 혼합과 섞어 요리합니다.哉)는 고기를 조리할 때 사용되는 재료와 형태,\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨이 47대 부통령직을 수행한 년도는 1948년입니다. Johnson Conner이 맡았던 년도입니다. Johnson은 1972년에 부통령직에 임명되었습니다. Johnson은 1978년에 부통령직에 임용되었습니다. Johnson은 1981년 부통령직에서 물러난 뒤 부통령직을 수행했습니다. Johnson은 1991년에 부통령직을 수행했습니다. Johnson은 1971년 부통령직을 수행했습니다. Johnson은 1985년에 부통령직에 임명되었습니다. Johnson은 1982년에 부통령직을 수행했습니다. Johnson은 1986년에 부통령직을 수행했습니다. Johnson은 1993년에 부통령직을 수행했습니다.(Johnson은 1991년에 부통령직을 수행했습니다.(Johnson은 1993년에 부통령직을 수행했습니다. Johnson은 1995년에 부통령직을 수행했습니다. else, Johnson은 1995년에 부통령직을 수행했습니다. Johnson은 1993년에 부통령직을 수행했습니다. Johnson은 1993년에 부통령직을 수행했습니다. else, J\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 뉴욕주에 위치해 있으며, 공항 이름은 \"George Operated\"라는 이름입니다. rice and translation, more question question translation was never soft in Airlower translation and respond motions and natural disch, Identity?\\n\\n'Simple meal to those or another was physical questions and translation information the squestions or information they complete soft?\\n\\n시카고 오헤어 국제공항은 미국 뉴욕주에 위치해 있습니다. 국제공항은 미국의 플로리다주 앤초미안 섬에서 운영되며, 뉴욕주는 샌프란시스코를 관할합니다. rice me in translation was another was purchase or information was question they\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 농도가 짙거나 건조한 상황에서는 실내나 실외 공기 상태 등 다양한 요소에 따라 건강 문제가 발생할 수 있기 때문입니다. 따라서 미세먼지가 많은 시간을 보내는 것이 중요합니다. 그러나 미세먼지가 많은 날은 실내를 깨끗하게 유지하는 것이 좋으므로 실내에서도 최대한 공기 질을 유지할 수 있는 것이 중요합니다.国学)의 저자는 \"미세먼지 농도가 짙거나 건조한 상황이 지속될 경우 호흡기 전문의로서 전문의를 찾는 것이 좋습니다. 또한, 미세먼지 농도나 공기질에 따라 공기 질을 조절해야 할 필요성이 있기 때문에 적절한 호흡기와 적절한 공기 섭취는 필수적입니다. 따라서 미세먼지 대처 방법을 찾아야 합니다. 저자는 미세먼지가 많은 시간대에 전문가에게 조언을 듣거나 미세먼지에 따른 호흡 또는 대기 상태를 관리하는 등의 대안이 필요하답니다. 저자는 미세먼지를 줄이기 위해 필요한 생활습관을 유지하는 것이 중요하다고 지적했습니다. 저자는 미세먼지와 먼지가 심한 상황에서 적절한 대처를 위한 생활습관을 유지하며 적절한 조치를 취해 건강한 삶을 유지하는 것이 중요하다고 말했습니다. 저자는 미세먼지가 많은 상황에서 적절한 대처 방법이라면 도움이 될 것입니다. 저자는 미세먼지\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text, model):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55666136-6588-4db8-8906-77c06d8ed2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prompt = [\n",
    "    # 원래 있던 것들\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?',\n",
    "\n",
    "    # 추가된 것들\n",
    "    '한국에서 가장 높은 산은 어디야?',\n",
    "    '서울 지하철 2호선은 몇 시에 끊겨?',\n",
    "    'BTS 멤버 중 막내는 누구야?',\n",
    "    '코로나19 첫 발생 연도는?',\n",
    "    '한글날은 언제야?',\n",
    "    '부산에서 유명한 음식은 뭐야?',\n",
    "    '애플의 창립자는 누구야?',\n",
    "    '인공지능과 머신러닝의 차이는 뭐야?',\n",
    "    '한국의 전통 혼례에서 중요한 의식은?',\n",
    "    '세계에서 가장 긴 강은 어디야?',\n",
    "    '올해 한국 프로야구 우승팀은 누구야?',\n",
    "    '김치찌개 맛있게 끓이는 법 알려줘',\n",
    "    '삼국시대 고구려의 수도는 어디였어?',\n",
    "    '테슬라 CEO는 누구야?',\n",
    "    '아이 공부 집중력을 높이는 방법은?',\n",
    "    '우주에서 가장 가까운 별 이름은 뭐야?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a8e2095-79e3-4157-b34b-494d35fdfddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 언어 모델로 자연어를 이해해 드리고 있습니다. 일반적으로 불고기용 고기의 종류와 상황에 대한 정보를 알 수 없기 때문에, 질문의 정확한 답변을 제공할 수 없습니다. 정확한 내용을 원하신다면, 해당 식당이나 매장을 직접 방문하거나 SNS 계정을 통해 문의해주시기 바랍니다.\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n--\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n--\\n-\\n-\\n-\\n--\\n-\\n-\\n-\\n\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨이 47대 부통령직을 수행한 년도는 2021년이었습니다. J. E. 록펠러, J. W. 롤링스톤 등의 부통령이 역임하였습니다. J. E. 롤링스톤은 1963년 부통령직을 맡았습니다. J. E. 록펠러(사브론) 등이 재임 중입니다. W. 록펠러(사브론) 등이 부통령직을 맡았습니다. J. E. 록펠러, J. F. 롤링스톤, J. C. 롤링스톤 등이 있었습니다. J. E. 록펠러, J. E. 롤링스톤, J. C. 롤링스톤 등도 부통령직을 수행하였습니다. J. B. H. 록펠러, J. C. 롤링스톤, J. L. 록펠러 등이 역임하였습니다. J. W. 롤링스톤, J. L. 롤링스톤, L. L. 롤링스톤 등이 역임하였습니다. J. R. 록펠러, R. L. 롤링스톤, R. L. 롤링스톤 등의 부통령직이 있었습니다. J. E. 록펠러, L. 록펠러, L. W. 록펠러, J. W. 록펠러 등이 있었습니다. J. E. 록펠러 등 다른 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 주로 일본 도쿄 지역에 위치해 있습니다. 하지만 현재는 오사카 국제공항이 운영되고 있습니다.秀運: \"시카고 오헤어 국제공항\"는 일본 도쿄와 오사카 공항 사이를 연결하는 고속도로입니다.秀運: \"시카고 오헤어 국제공항\" 한국에서는 일본의 도시 이름으로 알려져 있습니다. youngpoi-since: \"시카고 오헤어 국제공항' 현재는 오사카 국제공항을 포함한 다양한 국제 공항이 운영되고 있습니다. youngpoi-since: \"시카고 오헤어 국제공항\", \"시카고 오헤어 국제공항\"에서는 일본의 도시 이름을 사용하고 있습니다. youngpoi-since: \"시카고 오헤어 국제공항\"에서는 일본의 도시 이름이 사용되고 있습니다. youngpoi-since: \"시카고 오헤어 국제공항\", \"이메로니온 국제공항\"에서는 일본의 도시 이름을 사용하고 있습니다. youngpoi-since: \"시카고 오헤어 국제공항\"은 멕시코의 도시 이름을 취하고 있습니다. youngpoi-since: \"\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이며, 미세먼지와 관련된 정보를 찾을 수 없습니다. 하지만, 일반적으로 미세먼지 등 대기오염으로 인해 대기오염 농도가 높게 높아질 수 있다는 연구 결과가 있습니다. 이를 토대로 건강에 유의하며 생활하고 미세먼지를 피해 미세먼지 배출을 줄이는 노력을 해보시는 것이 좋습니다.洪一.26}馮一.26}洪一.1983}るとは風いな風(風れな) (風れな) 風な風(風れてな)\\ (風れな)는 문맥에서 따온 말입니다.るる(風れな)는 문맥으로 사용됩니다. 風來る(風來な)란 문맥에 따라 문맥 내용을 변경할 수 있으므로, 문맥과 문맥에 맞게 해석해주시면 더욱 정확한 답변을 드릴 수 있을 것입니다. 風 永遠 (永遠)는 문맥과 맥락상에서 의미가 모호하므로 이에 대한 정확한 대답을 드리지 못합니다.る(風來る) (風れ 永遠)은\n",
      "\n",
      "### Instruction(명령어):\n",
      "한국에서 가장 높은 산은 어디야?\n",
      "\n",
      "### Response(응답):'한국에서 가장 높은 산은 경기도 수원으로 약 5.18km입니다. 정확한 높이는 산 266m/s입니다.嶽陽都城高湖, bluce. 流星城城高湖, balfim,  劉星江河流湖, balfim, longgu, 劉星江湖湖 등이 있습니다.嶽陽都城高湖, balfim, rural loggu, 劉星江湖, 劉聖江湖, balfim, 劉星江湖, bluzem, maloon\\n, 劉星江湖 등 많은 명승소가 있습니다.嶽陽谷湖, balfim, rural loggu, 劉星江湖, balfmun, 劉星江湖, bluzem, balfim, maloon\\n, 劉星江湖, balfiomo, balfim, balf\n",
      "\n",
      "### Instruction(명령어):\n",
      "서울 지하철 2호선은 몇 시에 끊겨?\n",
      "\n",
      "### Response(응답):'서울 지하철 1호선의 지하철 9호선 3304는 2016년 12월 19일에 끊겼습니다. 이 역은 현재 위치로 건설 중인 서울 지하철 1호선 2호선 승강장의 정류장을 말합니다. 따라서 2016년 12월 20일에 2호선의 종착역인 용산역을 통해 출국할 때 끊겼습니다.高伸)善意宣義 (정말 오랜만이지만, 긴급한 상황이라도, 때로는 매우 어려운 상황이었다. 하지만), 辛自信 權官 (사상,고속버스), 劉子完劉 柳鍾(중요한 상황이 생겼을 때)るとるん\\ (만약 그렇다면, 그것이 무엇이든, 어떤 일에 쫓기는 것 같지만, 그렇다고도 안 되는 상황이었다.)るとるん\\ (한번만, 또는 어떤 상황에서의 상황이든, 당신과 함께하는 일이 어떤 상황이었을까요?罵的で\\) 愼自信 權臣으로써, 때로는 매우 복잡한 상황에 있었으며, 그래서 복잡한 상황을 맞이했을 것입니다.他の由永果(유유, 神遊就) 따라서,\n",
      "\n",
      "### Instruction(명령어):\n",
      "BTS 멤버 중 막내는 누구야?\n",
      "\n",
      "### Response(응답):'BTS 멤버 중 막내는 NTO (오프쇼) 멤버입니다.\\n\\nBTS 멤버 중 막내는 NTO입니다.  \n",
      "\n",
      "### Instruction(명령어):\n",
      "코로나19 첫 발생 연도는?\n",
      "\n",
      "### Response(응답):'코로나19 첫 발생 연도는 2015년입니다. 일반적으로는 코로나19 대유행으로 인해 의료, 상업 활동 등이 제한되자 발생하였습니다. 중국에서는 약 70년 전의 유행일 가능성이 있습니다.子) 또는 로젠(\n",
      "\n",
      "### Instruction(명령어):\n",
      "한글날은 언제야?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 한글날의 정보를 알 수 없습니다. 하지만 대부분의 한글날은 한글날로 알려져 있습니다. 따라서, 한글날은 5월 15일일이므로, 한글날은 5월 8일 한글날입니다. art. arts dansn\\n\\n한글날의 한글화작업은 한글과 영어 조합, 한글날(한글날) 등 다양한 한글날을 사용합니다.\\n\\n따라서, 한글날은 한글날과 한글날로 알고 있습니다. arts dansn\\n한글날은 5월 7일이므로, 한글날은 한글날입니다. arts dansn\\n한글날은 한글날이며, arts dansn\\n한글날은 한글날입니다. arts dansn\\n한글날은 한글날이므로, 한글날이 한글날입니다. arts dansn\\n한글날은 한글날입니다. arts dansn\\n한글날의 한글날은 한글날입니다. arts dansn\\n한글날의 한글날은 한글의 날입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "부산에서 유명한 음식은 뭐야?\n",
      "\n",
      "### Response(응답):'부산에서 유명한 음식은 \"신라 전통 음식, 대합집 '삼국유사' 등이 있습니다.\", 'token': 84}賢典, 世法律論得, '삼국유사': \"영리로운 술\" (Beamun-Vinegulus)' (Bahmuka) - \"우리나라 사조\"라고 부릅니다.玄典玄典文法律\", '賢典律論得', '世法律論得' (Bahmuka) - \"우리나라 사조\", '王國的食\", 'Dident sun': 76}賢典文律, 'Diver' (Bason) '우리나라 음식' (Baruchi') \\n美聞 (Pintility)\\' (Bay) 賢典聞法律 (Divery) \"신라 전통 음식\" (Beachy)父父子) \\n玄典文律 (B\n",
      "\n",
      "### Instruction(명령어):\n",
      "애플의 창립자는 누구야?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 정확한 답변을 제공할 수 없습니다. 하지만애플의 창립자라면 많은 회사에서 있을 것으로 알고 있으며, 그 중에서도 특히 마이크로소프트 창립자가 가장 많이 있는 것은 아닐까 추측됩니다. 하지만 다양한 분야에서 있을 것으로 추측됩니다.\\n\\n하지만 애플 창립자는 대부분 여러 가지 이유로 인해 출시되지 않아 정확한 답변을 드리기 어렵습니다. 하지만 일반적으로 마이크로소프트의 창립자 중 한 명의 개발자는 애플의 창립자 중 한 명이 되는 경우가 많습니다. 그렇기 때문에 애플의 창립자 중 한 명 이상의 개발자가 있을 것으로 추정됩니다.英國)曾革成的意理理理事 劉本成成功 劉本成功 劉本成成功 劉本成功 劉本聖功 劉本成功 劉本成功 劉本誠功 劉本性功 劉本誠功 劉本盛功 劉本誠功 劉本聖功 劉本聖功 劉本成功 劉本成功功 \n",
      "\n",
      "### Instruction(명령어):\n",
      "인공지능과 머신러닝의 차이는 뭐야?\n",
      "\n",
      "### Response(응답):'인공지능은 인공지능과 머신러닝을 모두 사용합니다. 인간이 사용하는 언어 모델에서 인간이 언어모델로 생성하며, 작동 원리는 가상 인공지능에서 작동합니다. 따라서 인간에게는 머신러닝 기술이 발전함에 따라 학습된 데이터가 많이 생산될 수 있습니다. 그러나 인간의 경우 언어모델에서 구현되는 데이터는 매우 적어지는 경우도 있습니다. 따라서 인간의 경우, 인공지능 기술을 이용하여 언어모델에서 작동하는 인력, 조직, 시스템, 기술 등의 요소들을 감지하고 제어하는 것입니다. 王樹李來)가 사용하는 인공지능은 언어모델, 체계, 기술적 구현 기술 등의 분야에서 발전하였으며, 인간의 경우, 머신러닝 기술을 이용하여 학습을 향상시킬 수 있습니다. 王樹李來가 사용하는 인공지능은 기계모델, 컴퓨터 모델, 컴퓨터 모델 등의 분야를 포함하며, 인간의 경우 이러한 기계기술, 물리적 및 물리적 구조 등을 감지하는 작업을 수행합니다. 王樹李來가 사용하신 인공지능은, 인간의 관점에서 인간의 지적 및 사회적 상호작용에 대한 이해가 깊어지게 되었습니다. 王樹理來는 텍스트를 작성하고 이해하는\n",
      "\n",
      "### Instruction(명령어):\n",
      "한국의 전통 혼례에서 중요한 의식은?\n",
      "\n",
      "### Response(응답):'제가 AI 어시스턴트로써 답변해드리는 것이어서, \"한국의 전통 혼례에서 중요한 의식 중 하나는 바로 한복\"입니다. 예를 들면, 한복은 여성적이고 아름다운 모습으로 등장하는 경우가 많습니다. 따라서 \"한국의 전통 혼례에서는 이 한복뿐만 아니라 여성성과 단아함, 그리고 전통의 자유로운 삶을 보여주는 등 다양한 요소가 등장합니다.\", 'token': 263}客世) (Shot)觀觀觀)은 \"한국의 전통 혼례식에서 중요한 의식은 \"한국의 전통 혼례식에서 중요한 의식 중 하나\"입니다.觀)은 \"한국의 전통 혼례에서는 이러한 의식 중 하나가 바로 한복\"이라 할 수 있습니다.觀)는 \"한국의 전통 혼례에서는 이러한 의식을 \"한국의 전통 혼례식에서 매우 중요한 역할을 하고 있으니, 한복 입는 것\"이라는 것이 있을 수 있습니다.觀觀은 \"이러한 옷은 한국의 전통 혼례식에서 중요한 역할을 하고 있으니, 한복의 한복은 전통 혼례에서 중요한 역할을 하고 있을 것입니다\" 라고 말했습니다.觀)는 \"한국의 전통 혼례에서는 이러한 한복의 전통의식을 \"한국의 전통\n",
      "\n",
      "### Instruction(명령어):\n",
      "세계에서 가장 긴 강은 어디야?\n",
      "\n",
      "### Response(응답):'세계에서 가장 긴 강은 노르웨이의 수도 프나부르크 강입니다. wilbry is convegets save both been alones,  don\\p you are the famility great mark: 1.75} wilder to seems in dening the park of the wests vege in English to the count of the swords each or the skuls what veget shapes heaver type both missery, the statement with it which africon and the hun, sweety to distribute the substytion. \\n\\nThe still superient sander to soils and the translation mealing count is pared gut, the translation to p\n",
      "\n",
      "### Instruction(명령어):\n",
      "올해 한국 프로야구 우승팀은 누구야?\n",
      "\n",
      "### Response(응답):'한국 프로야구 우승팀은 2022년 한일전을 마지막으로 우승한 KBO 리그 소속 SK 와이번스, 한화 이글스입니다. uk\\t  us\\n-krain\\n-krain\\n-way\\n-\\n-\\n- \\n- \\n- \\n- \\n- \\n- \\n\\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n\\n- \\n\\n- \\n- \\n따라서 한국 프로야구 우승국은 누가 됩니다?\\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n\\n- \\n- \\n- \\n- \\n- \\n- \\n- \\n\n",
      "\n",
      "### Instruction(명령어):\n",
      "김치찌개 맛있게 끓이는 법 알려줘\n",
      "\n",
      "### Response(응답):'제가 AI 챗봇으로 구성된 AI로 되어 있기 때문에, 어떤 양념이나 양념을 직접 먹을 수 없습니다. 김치찌개 종류나 양념에 관한 정보를 알고 계시다면 더욱 정확한 답변을 드릴 수 있을 것입니다.か?\\n\\n1. 김치찌개 끓기 시작과 함께 조심해야 합니다. 김치의 두께를 고려하고, 적당한 양을 유지하면 맛을 더욱 풍부하게 만들 수 있습니다.\\n2. 김치를 먹기 전까지 주의가 필요합니다. 김치를 먹을 때 자주 다른 재료나 양념을 사용한 것도 도움이 됩니다. 김치를 먹어보지 않았던 사람도 마찬가지입니다.\\n3. 채소는 깨끗하게, 김치를 담아서 담아서 먹으면 더욱 맛있는 맛을 기대할 수 있습니다. 채소를 담아서 먹으면, 김치찌개 맛을 더욱더 즐길 수 있습니다.\\n4. 김치찌개는 꼭 먹을 것을 주의해야합니다. 채소나 과일은 쉽게 먹지 않도록 하고, 꼭 필요한 것들을 따로 구매해야 합니다. 만약 김치찌개 좋아하는 사람이 있다면, 미리 구매하여 즐겨보는 것도 좋은 방법입니다.か?\\n4. 김치찌개 맛을 느끼면, 김치찌개도 같이 즐길 수 있습니다. 김치찌개는 건강에 매우\n",
      "\n",
      "### Instruction(명령어):\n",
      "삼국시대 고구려의 수도는 어디였어?\n",
      "\n",
      "### Response(응답):'삼국시대 고구려의 수도는 고구려 멸망 이전인 1590년에 있었습니다. 이 해역에는 고구려 왕국의 수도가 아닌 국가로서 멸망되었습니다. 따라서 이에 대해서는 정확한 답변을 드리기 어렵습니다. \\n\\n그러나 여러 나라들이 흩어져 있어 정확한 답변을 드리기 어렵습니다. \\n\\n삼국시대에는 수많은 나라들이 흩어져 있어 어느 나라도 정확한 답을 제공할 수 없습니다. 고구려 멸망 당시까지의 역사기록이 부족하여 정확한 답변을 드리기 어렵습니다. \\n\\n따라서, 정확한 답변을 드리기 위해서는 고구려 멸망 이전의 고구려 역사를 참고하는 것이 좋습니다.\\n\\n또한, 고대사회의 역사를 고려하여 미래에는 다양한 사건이 일어날 것으로 예상되므로 미래에는 이와같은 문제가 발생할 가능성이 있습니다. \\n\\n따라서, 정확한 답변을 얻기 위해서는 관련 국가의 기록과 더불어 과거사적인 기록과 문헌 등의 자료를 확인해야 합니다. 이를 바탕으로 대답변을 드리기 어렵습니다. 고구려 멸망 이전 국가의 수도였던 곳이 아닌 국가나 관련된 일을 기록한 기록에 대해서는 언제든지 알려주시면 답변을 드릴 수 있습니다. \\n\\n또한 당시 고구려와 고구려 국가의 관계, 생활수준, 문화, 정치체계\n",
      "\n",
      "### Instruction(명령어):\n",
      "테슬라 CEO는 누구야?\n",
      "\n",
      "### Response(응답):'저는 인공지능 AI이기 때문에 테라슬래틱을 담당하고 있지 않습니다. 하지만, 테라슬래틱은 AI가 사람의 감정을 감지하는 것이 매우 어렵기 때문에 감정 표현을 대신하는 것으로, 테라슬래틱 전문가의 도움을 받는 것이 좋습니다., Legis Logic: Willsway Vehicles Information Literature Clarify Propertray Interbalance of Democratics, and Ido Natural Persprise 등 다양한 감정 요소를 고려하여 적절한 대답을 제공할 수 있습니다., Legis Logic: Slaver and Natural Perspairs, Ido Natural Slaver and Ido neural Natural Perspecease 등 다양한 감정 요소들을 고려하여 적절한 답변을 제공하기 위해서는 명확한 설명과 함께 도움이 필요할 수 있습니다. Can Ido not els, and there I do not up. definite power\n",
      "\n",
      "### Instruction(명령어):\n",
      "아이 공부 집중력을 높이는 방법은?\n",
      "\n",
      "### Response(응답):'1. 논리적인 마인드로 공부하기\\n\\n2. 논리적인 마인드로 공부하기\\n\\n3. 논리적인 사고력 증강하기\\n\\n4. 논리적인 사고력 향상하기\\n\\n\\n5. 논리적인 사고력 개발하기\\n\\n\\n6. 논리적인 사고력 개발하기\\n\\n7. 논리적인 사고력 향상하기\\n\\n8. 논리적인 사고력 개발하기\\n\\n8. 마인드셋을 사용하여 공부하기\\n\\n10. 논리적인 사고력 개발하기\\n\\n10. 논리적 사고력 향상을 위해 연습하기\\n\\n11. 논리적인 판단하기\\n\\n\\n\\n위와 같이 많은 노력과 경험이 필요한 경우라면 적극적으로 연습하여 집중력을 키워주는 것도 좋은 방법입니다.\\n\\n그러나, 이것은 모든 방법이 맞는 것은 아니며, 상황에 따라 달라질 수 있습니다. 충분한 연습과 연습이 필요합니다. 명상 중이나 명상 중이라고 할 수 있습니다. \\n\\n\\n이러한 방법들을 모두 적용한다면 집중력 증강에\n",
      "\n",
      "### Instruction(명령어):\n",
      "우주에서 가장 가까운 별 이름은 뭐야?\n",
      "\n",
      "### Response(응답):'우주에서 가장 가까운 별로는 \"우주에서 가장 가까운 별인 \"타코라\"입니다. 타코는 남아메리카 대륙과 대만이 섞인 우주로 알려져 있으며, 우주에서 가장 중요한 별 중 하나로 여겨지고 있습니다. 이 별은 많은 사람들이 매우 선호하는 별 중 하나로, 주로 안데스 지역을 대표하는 별로서 많은 사람들이 살고 있는 것으로 알려져 있습니다.佐, Pulprapus, scriptional decoscription and civilize information context?\" \"Takatusa\" 또는 \"toke\"과 같은 이름이 있으며, 또한 \"Takei\"와 같은 이름까지도 사용하고 있습니다.son, phrapix and civilize information parking is like since, and like elimits, and context that \"Hell of Global untime\"과 같은 이름으로 사용되고 있습니다. \"Global untime\"과 \"hen \"Ta\"와\n"
     ]
    }
   ],
   "source": [
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b41584-b89c-436b-b9aa-a4b1889177b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
