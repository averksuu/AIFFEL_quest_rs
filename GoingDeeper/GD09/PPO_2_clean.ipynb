{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG5NtAKnuV8_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xukndR53uqiO"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QWLk-i1us9D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_c9bws5u0c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct12katcwFQo"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/output_2_RM.zip' -d output_RM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOFcQ8dLIBZ_"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/KoChatGPT-20250919T020728Z-1-001.zip' -d sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "a2bd84c25bce4495a92f2671783a0b8c",
      "d481ca70620d41009f6c10a25f3335ba",
      "a8734bf90ecf44409ba3e1197336d321",
      "15ca9e3b5ca44d34b7b60b9f53eaa244",
      "4ba57a568e4f470495193ddda2ff9666",
      "db7a9670832349c0acddd2a14c927479",
      "cf199da86a0f4372bb8265d0f6d2e7e0",
      "d37b79e010f34f59b51b5f382baea2ce",
      "e4ecdecf21594404a6d6d05c14bc8959",
      "b3eea2651641496faa988dbc3ad62c2d",
      "52275f4b46ef42e19c334828bfb10978",
      "4a1bf2f4c5ec49fbab272b85929353a8",
      "d5ba4e6f151044c2b0588a2a97e06684",
      "62f0f1876c024e249d15e68afecb9f85",
      "fb03f2bc9bca46d38418dbee9cdb9cdc",
      "b6a0a84d7aab4d2ba5fac1bc826e54f4",
      "7a4da75c417447d4b28fe2601a4ef093",
      "bd23904eddae4c0c8a4eb2bdf3d40f56",
      "2f38e5cd56e941efbd2387efb27d66f8",
      "d49ed20fc6734a1f86a9f76cc0790f0c",
      "bb9b435c5d2c474491b85c097654da0c",
      "0bd4be19c4b540639ab4e195c6ccd1ab",
      "43c9aca5392f468b918c3972310e706a",
      "4189a99392c245b8bc457bbd4044a9a1",
      "f9003819af044659a9b3891b151f346d",
      "3a8011fbdf5445959165986a73c9c031",
      "cb27363faf3f41b28c79b99d51d21079",
      "b84998479b94470897f956756b32c147",
      "9294d2aa204b49c4bba84cbe57c09292",
      "3ff7890804a3497fa57c5f30f5c89cc7",
      "4d4f6434d2d444739d58be76e861d313",
      "9aa544a0b1ee4198b9dcdbb67dcee600",
      "79cf98ea5933491d934895ff3dc08f52",
      "c4296387565c439f832da57ca839cadb",
      "73f5c672941b4096ad66a7266b5cf6b1",
      "210ee6cbeab2463bb06c50eafaa178b0",
      "61ce97e8ca224876875e079584a9b20d",
      "f4f1bf5bd4ad4def8863a29dcd304e52",
      "e46c064dcd4e4702a7c98dd45f39aa60",
      "c3147ebaac5044a3b58cd5c37c800d72",
      "3ee4a9fad56249108b7cf976b169b6fc",
      "651039cdca1b4bfda7fa922775d038d6",
      "1692835302af4e9caf38641ed205a880",
      "97249e72f3244754bd7a8514cbbc874f",
      "f0ec0c74caa54e7991447be183363678",
      "014fb4ecf10a401aba5790b1b081c766",
      "deebed32c18040849e7946e8ecd5f8c6",
      "02f813c62d284c838094d1473374dcbc",
      "f820bf2f7ff14710a279cec7b6162b7b",
      "a7259226d85049bb8080f8380c81c103",
      "45966535630d4e799831515dcee31c95",
      "797c6e8735f14e2aad215add920ad74b",
      "7e9fb229fbf848559d74dd3e3a1105ca",
      "c3a1337e5b1c43a2a9e956eeb6fef62d",
      "72519519aa344885a873cc77de58f731"
     ]
    },
    "id": "yZhpvAGW69Qk"
   },
   "outputs": [],
   "source": [
    "# === PPO step: actor-only (REINFORCE + KL к initial), награда из замороженного RM ===\n",
    "import os, json, random, torch\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n",
    "from peft import PeftModel\n",
    "\n",
    "# ===================== ПУТИ =====================\n",
    "SFT_CHECKPOINT      = \"/content/sft/KoChatGPT/output_SFT_trinity345M_dynpad\"\n",
    "RM_CHECKPOINT_DIR   = \"/content/output_RM\"\n",
    "BASE_MODEL_ID       = \"skt/kogpt2-base-v2\"\n",
    "PPO_OUTPUT_DIR      = \"/content/drive/MyDrive/KoChatGPT/output_PPO_actor\"\n",
    "\n",
    "DATA_JSON           = \"KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl\"  # обычный JSON list[{\"prompt\":...}]\n",
    "\n",
    "os.makedirs(PPO_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===================== SAFE INIT =====================\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def safe_set_seed(seed: int = 42):\n",
    "    import numpy as np\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        except Exception as e:\n",
    "            print(\"[warn] CUDA seeding failed:\", repr(e))\n",
    "\n",
    "safe_set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ===================== PROMPT FORMAT =====================\n",
    "INSTR = \"### Instruction:\\n\"\n",
    "RESP  = \"\\n\\n### Response:\\n\"\n",
    "def format_prompt(p: str) -> str:\n",
    "    return f\"{INSTR}{p}{RESP}\"\n",
    "\n",
    "# ===================== TOKENIZERS =====================\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_CHECKPOINT)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ===================== ACTOR (LoRA PEFT) =====================\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "actor = PeftModel.from_pretrained(base_model, SFT_CHECKPOINT)\n",
    "if hasattr(actor, \"peft_config\") and \"default\" in actor.peft_config:\n",
    "    actor.peft_config[\"default\"].inference_mode = False\n",
    "actor.to(device)\n",
    "\n",
    "# учим только LoRA (и lm_head по желанию)\n",
    "for n, p in actor.named_parameters():\n",
    "    p.requires_grad = (\"lora\" in n.lower()) or (\"lm_head\" in n)\n",
    "\n",
    "print(\"[actor] total params:\", sum(p.numel() for p in actor.parameters()),\n",
    "      \"| trainable:\", sum(p.numel() for p in actor.parameters() if p.requires_grad))\n",
    "\n",
    "# якорь для KL\n",
    "initial_model = deepcopy(actor).to(device).eval()\n",
    "for p in initial_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optim_params = [p for p in actor.parameters() if p.requires_grad]\n",
    "assert optim_params, \"Нет trainable-параметров у актора\"\n",
    "actor_optim = torch.optim.AdamW(optim_params, lr=5e-5)\n",
    "\n",
    "# ===================== REWARD MODEL =====================\n",
    "# токенизатор RM\n",
    "try:\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(RM_CHECKPOINT_DIR)\n",
    "    print(\"[RM] tokenizer loaded from RM dir\")\n",
    "except Exception:\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "    print(\"[RM] tokenizer fallback to BASE_MODEL_ID\")\n",
    "rm_tokenizer.padding_side = \"left\"\n",
    "if rm_tokenizer.pad_token is None:\n",
    "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
    "\n",
    "# backbone RM\n",
    "rm_backbone = GPT2Model.from_pretrained(RM_CHECKPOINT_DIR)\n",
    "\n",
    "# resize embeddings если токенизатор длиннее\n",
    "model_vocab = rm_backbone.get_input_embeddings().num_embeddings\n",
    "tok_vocab = len(rm_tokenizer)\n",
    "if tok_vocab != model_vocab:\n",
    "    print(f\"[RM] resize embeddings {model_vocab} -> {tok_vocab}\")\n",
    "    rm_backbone.resize_token_embeddings(tok_vocab)\n",
    "\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state[:, -1, :]\n",
    "        return self.value_head(last_hidden).squeeze(-1)\n",
    "\n",
    "hidden = rm_backbone.config.n_embd\n",
    "reward_model = SimpleRewardModel(rm_backbone, hidden).to(device).eval()\n",
    "vh_path = os.path.join(RM_CHECKPOINT_DIR, \"value_head.bin\")\n",
    "if os.path.exists(vh_path):\n",
    "    reward_model.value_head.load_state_dict(torch.load(vh_path, map_location=\"cpu\"))\n",
    "else:\n",
    "    print(\"⚠️ value_head.bin не найден — RM head случайная\")\n",
    "\n",
    "for p in reward_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# preflight\n",
    "with torch.no_grad():\n",
    "    enc = rm_tokenizer(\"### Instruction:\\n테스트\\n\\n### Response:\\n좋아\", return_tensors=\"pt\")\n",
    "    max_id = int(enc[\"input_ids\"].max())\n",
    "    vocab  = reward_model.backbone.get_input_embeddings().num_embeddings\n",
    "    print(f\"[RM] preflight: max_id={max_id}, vocab={vocab}\")\n",
    "    _ = reward_model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
    "    print(\"[RM] smoke ok\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def rm_score_texts(text_batch, max_len=512):\n",
    "    enc = rm_tokenizer(text_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    return reward_model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
    "\n",
    "# ===================== DATA =====================\n",
    "def load_prompts_from_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        obj = json.load(f)\n",
    "    if isinstance(obj, list):\n",
    "        records = obj\n",
    "    elif isinstance(obj, dict):\n",
    "        records = obj.get(\"data\") or obj.get(\"items\") or []\n",
    "    else:\n",
    "        raise ValueError(\"Bad JSON format\")\n",
    "    prompts = [r.get(\"prompt\",\"\").strip() for r in records if isinstance(r,dict) and r.get(\"prompt\")]\n",
    "    seen, uniq = set(), []\n",
    "    for p in prompts:\n",
    "        if p not in seen: uniq.append(p); seen.add(p)\n",
    "    random.shuffle(uniq)\n",
    "    return uniq\n",
    "\n",
    "list_prompt = load_prompts_from_json(DATA_JSON)\n",
    "print(\"Загружено промптов:\", len(list_prompt))\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def tokenize_inputs(texts, max_len=96):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    return {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "def logprobs_from_logits(logits, ids):\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    return torch.gather(logp, -1, ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=8,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# ===================== PPO LOOP =====================\n",
    "actor.train()\n",
    "beta_kl, adaptive_kl, kl_target, kl_lr = 0.01, True, 0.1, 0.1\n",
    "BATCH_SIZE, UPDATES = 8, 100\n",
    "pbar = tqdm(range(UPDATES), desc=\"PPO\")\n",
    "\n",
    "for it in pbar:\n",
    "    s = (it * BATCH_SIZE) % max(1, len(list_prompt) - BATCH_SIZE + 1)\n",
    "    batch_prompts = [format_prompt(p) for p in list_prompt[s:s+BATCH_SIZE]]\n",
    "    if not batch_prompts: break\n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        inp = tokenize_inputs(batch_prompts, max_len=96)\n",
    "        gen_ids = actor.generate(**inp, **gen_kwargs)\n",
    "    actor.train()\n",
    "\n",
    "    prompt_len = inp[\"input_ids\"].shape[1]\n",
    "    attn_all   = (gen_ids != tokenizer.pad_token_id).long()\n",
    "    gen_part_ids = gen_ids[:, prompt_len:]\n",
    "    gen_mask     = (gen_part_ids != tokenizer.pad_token_id).float()\n",
    "\n",
    "    actor_out = actor(input_ids=gen_ids, attention_mask=attn_all)\n",
    "    init_out  = initial_model(input_ids=gen_ids, attention_mask=attn_all)\n",
    "\n",
    "    actor_logits = actor_out.logits[:, :-1, :]\n",
    "    init_logits  = init_out.logits[:,  :-1, :]\n",
    "    target_ids   = gen_ids[:,    1:]\n",
    "\n",
    "    actor_logits_gen = actor_logits[:, prompt_len-1:, :][:, :gen_mask.shape[1], :]\n",
    "    init_logits_gen  = init_logits[:,  prompt_len-1:, :][:, :gen_mask.shape[1], :]\n",
    "    target_ids_gen   = target_ids[:,   prompt_len-1:][:, :gen_mask.shape[1]]\n",
    "\n",
    "    logp_actor = logprobs_from_logits(actor_logits_gen, target_ids_gen)\n",
    "    logp_init  = logprobs_from_logits(init_logits_gen,  target_ids_gen)\n",
    "\n",
    "    kl_mean = ( (logp_actor - logp_init) * gen_mask ).sum() / gen_mask.sum().clamp(min=1)\n",
    "\n",
    "    decoded = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "    rewards = rm_score_texts(decoded)\n",
    "\n",
    "    adv = (rewards - rewards.mean()) / (rewards.std(unbiased=False)+1e-6)\n",
    "    logp_actor_sum = (logp_actor * gen_mask).sum(dim=1)\n",
    "\n",
    "    policy_loss = -(adv * logp_actor_sum).mean()\n",
    "    loss = policy_loss + beta_kl * kl_mean\n",
    "\n",
    "    actor_optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(optim_params, 1.0)\n",
    "    actor_optim.step()\n",
    "\n",
    "    if adaptive_kl:\n",
    "        ratio = (kl_mean.item() / max(1e-8, kl_target)) - 1.0\n",
    "        beta_kl = float(max(1e-5, min(1.0, beta_kl * (1.0 + kl_lr * ratio))))\n",
    "\n",
    "    pbar.set_postfix({\"loss\":f\"{loss.item():.3f}\",\"reward\":f\"{rewards.mean().item():.3f}\",\"kl\":f\"{kl_mean.item():.3f}\",\"beta\":f\"{beta_kl:.4f}\"})\n",
    "\n",
    "# ===================== SAVE =====================\n",
    "actor.save_pretrained(PPO_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUTPUT_DIR)\n",
    "print(\"✅ Saved PPO actor (LoRA) to:\", PPO_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQchRuLP_XLO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ===== ТЕСТОВЫЕ ЗАПРОСЫ =====\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?',\n",
    "    '한국에서 가장 높은 산은 어디야?',\n",
    "    '서울 지하철 2호선은 몇 시에 끊겨?',\n",
    "    'BTS 멤버 중 막내는 누구야?',\n",
    "    '코로나19 첫 발생 연도는?',\n",
    "    '한글날은 언제야?',\n",
    "    '부산에서 유명한 음식은 뭐야?',\n",
    "    '애플의 창립자는 누구야?',\n",
    "    '인공지능과 머신러닝의 차이는 뭐야?',\n",
    "    '한국의 전통 혼례에서 중요한 의식은?',\n",
    "    '세계에서 가장 긴 강은 어디야?',\n",
    "    '올해 한국 프로야구 우승팀은 누구야?',\n",
    "    '김치찌개 맛있게 끓이는 법 알려줘',\n",
    "    '삼국시대 고구려의 수도는 어디였어?',\n",
    "    '테슬라 CEO는 누구야?',\n",
    "    '아이 공부 집중력을 높이는 방법은?',\n",
    "    '우주에서 가장 가까운 별 이름은 뭐야?'\n",
    "]\n",
    "\n",
    "# ===== ФОРМАТ ПРОМПТА (как при обучении) =====\n",
    "def format_prompt(p: str) -> str:\n",
    "    return f\"### Instruction:\\n{p}\\n\\n### Response:\\n\"\n",
    "\n",
    "# ===== НАСТРОЙКИ ГЕНЕРАЦИИ =====\n",
    "# режим 1: детерминированный (greedy) — для быстрой sanity-проверки\n",
    "gen_kwargs_greedy = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# режим 2: стохастический (sampling) — если greedy даёт мало текста\n",
    "gen_kwargs_sampling = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "def generate_batch(prompts, gen_kwargs):\n",
    "    texts = [format_prompt(p) for p in prompts]\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = actor.generate(**enc, **gen_kwargs)\n",
    "    return [tokenizer.decode(ids, skip_special_tokens=True) for ids in out_ids]\n",
    "\n",
    "# ===== ЗАПУСК ПРОВЕРКИ =====\n",
    "print(\"=== GREEDY (do_sample=False) ===\")\n",
    "greedy_out = generate_batch(list_prompt, gen_kwargs_greedy)\n",
    "for p, r in zip(list_prompt, greedy_out):\n",
    "    print(\"=\"*60)\n",
    "    print(\"Prompt:\", p)\n",
    "    print(\"Response:\", r)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
