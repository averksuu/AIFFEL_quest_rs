{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofplO8YN9TrI"
   },
   "source": [
    "변경 사항 요약\n",
    "\n",
    "LoRA 적용\n",
    "\n",
    "단순히 GPTActor, GPTCritic을 불러오는 대신, SFT 체크포인트에 저장된 LoRA 어댑터를 불러와서 actor 모델에 적용했음.\n",
    "\n",
    "학습 시에는 LoRA 레이어만 requires_grad=True로 설정하여 경량 학습이 가능하게 변경했음.\n",
    "\n",
    "Tokenizer 설정 수정\n",
    "\n",
    "예제 코드에서는 padding_side=\"right\"를 사용했는데, 우리는 **decoder-only 모델 특성에 맞게 padding_side=\"left\"**로 수정했음.\n",
    "\n",
    "PAD 토큰도 EOS 토큰으로 지정해서 안정적으로 학습되도록 조정했음.\n",
    "\n",
    "Reward Model 로딩 방식 변경\n",
    "\n",
    "예제에서는 critic 모델을 그대로 불러왔지만, 우리는 Reward Model을 별도로 저장된 checkpoint에서 backbone + value_head로 분리 로딩했음.\n",
    "\n",
    "토크나이저와 embedding 크기가 맞지 않을 경우 resize_token_embeddings()를 적용해서 충돌을 방지했음.\n",
    "\n",
    "PPO 루프 구현 차이\n",
    "\n",
    "예제에서는 PPOTrainer를 바로 호출했지만, 우리는 직접 PPO 루프(logprobs_from_logits, KL 보정, reward 계산, advantage 정규화 등)를 작성해서 더 세밀하게 제어했음.\n",
    "\n",
    "KL 보정도 adaptive KL로 동적으로 가중치를 조정하도록 구현했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG5NtAKnuV8_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xukndR53uqiO"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QWLk-i1us9D"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_c9bws5u0c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct12katcwFQo"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/output_RM_backup (1).zip' -d output_RM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "122a9b919cbe49cba9e44ce8bf649fb2",
      "929e74f32eb743f3941290bbde652943",
      "858072a6745f469c8364b44c1b0c7488",
      "82b8cc2a41d347ce97d4095f85f72b92",
      "8f61c33cbae34e99b99d4cc74fde39aa",
      "62b3153a9275468f819c2f6056169727",
      "043e82cb2ddb445cbfed4cb09d13ea8d",
      "c90c0738b5874ba784b2b69b87acbc3d",
      "824a33f19d574e10a2eb5773929afde2",
      "ea171bd608bd4084943a70324be5949c",
      "ccfbdd47fbf742faa4e6c9ca051cc915"
     ]
    },
    "id": "tIjxKSrwHprH"
   },
   "outputs": [],
   "source": [
    "# === PPO: actor-only (LoRA), с фиксами для RM (tokenizer/embeddings/mask) ===\n",
    "import os, json, random, torch, re\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------- ПУТИ --------------------\n",
    "SFT_CHECKPOINT    = \"/content/drive/MyDrive/KoChatGPT/output_SFT_trinity345M_dynpad\"  # LoRA SFT адаптер\n",
    "RM_CHECKPOINT_DIR = \"/content/output_RM\"                                              # RM: backbone + value_head.bin\n",
    "BASE_MODEL_ID     = \"skt/kogpt2-base-v2\"\n",
    "DATA_JSON         = \"KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl\"\n",
    "PPO_OUTPUT_DIR    = \"/content/drive/MyDrive/KoChatGPT/output_PPO_actor\"\n",
    "\n",
    "os.makedirs(PPO_OUTPUT_DIR, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# -------------------- SEED --------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        except Exception as e:\n",
    "            print(\"[seed warn]\", e)\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------- ФОРМАТ ПРОМПТА --------------------\n",
    "INSTR = \"### Instruction:\\n\"\n",
    "RESP  = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "def format_prompt(p: str) -> str:\n",
    "    # RM у тебя обучался на полном формате (инструкция+ответ),\n",
    "    # поэтому в PPO сохраняем тот же формат.\n",
    "    return f\"{INSTR}{p}{RESP}\"\n",
    "\n",
    "# -------------------- TOKENIZER (actor) --------------------\n",
    "# Используем тот, с которым ты SFT'ила адаптер (или совместимый)\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_CHECKPOINT, padding_side=\"right\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------- ACTOR (LoRA) --------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "actor = PeftModel.from_pretrained(base_model, SFT_CHECKPOINT).to(device)\n",
    "\n",
    "# Учим только LoRA (и при желании lm_head)\n",
    "for n, p in actor.named_parameters():\n",
    "    p.requires_grad = (\"lora\" in n.lower()) or (\"lm_head\" in n)\n",
    "\n",
    "# Якорная модель для KL\n",
    "initial_model = deepcopy(actor).to(device).eval()\n",
    "for p in initial_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optim_params = [p for p in actor.parameters() if p.requires_grad]\n",
    "assert optim_params, \"Нет trainable-параметров у актора (проверь загрузку LoRA)\"\n",
    "actor_optim = torch.optim.AdamW(optim_params, lr=1e-5, betas=(0.9, 0.999), weight_decay=0.0)\n",
    "\n",
    "# -------------------- REWARD MODEL (RM) --------------------\n",
    "# 1) токенизатор RM из чекпоинта RM; если нет — из BASE\n",
    "try:\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(RM_CHECKPOINT_DIR, padding_side=\"right\")\n",
    "    print(\"[RM] tokenizer loaded from RM checkpoint\")\n",
    "except Exception as e:\n",
    "    print(\"[RM] tokenizer fallback to BASE:\", repr(e))\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, padding_side=\"right\")\n",
    "if rm_tokenizer.pad_token is None:\n",
    "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
    "\n",
    "# 2) backbone RM\n",
    "rm_backbone = GPT2Model.from_pretrained(RM_CHECKPOINT_DIR)\n",
    "\n",
    "# 3) выравнивание словаря токенизатора и эмбеддингов модели\n",
    "v_tok = len(rm_tokenizer)\n",
    "v_emb = rm_backbone.get_input_embeddings().num_embeddings\n",
    "if v_tok != v_emb:\n",
    "    print(f\"[RM] resize_token_embeddings: {v_emb} -> {v_tok}\")\n",
    "    rm_backbone.resize_token_embeddings(v_tok)\n",
    "\n",
    "# 4) value_head + сборка RM\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state[:, -1, :]   # [B,H]\n",
    "        return self.value_head(last_hidden).squeeze(-1) # [B]\n",
    "\n",
    "reward_model = SimpleRewardModel(rm_backbone, rm_backbone.config.n_embd)\n",
    "vh_path = os.path.join(RM_CHECKPOINT_DIR, \"value_head.bin\")\n",
    "if os.path.exists(vh_path):\n",
    "    reward_model.value_head.load_state_dict(torch.load(vh_path, map_location=\"cpu\"))\n",
    "else:\n",
    "    print(\"⚠️ [RM] value_head.bin не найден — голова случайная (лучше загрузить обученную).\")\n",
    "\n",
    "reward_model.to(device).eval()\n",
    "for p in reward_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 5) sanity-прогон RM (чтобы поймать несоответствия до PPO)\n",
    "with torch.no_grad():\n",
    "    enc = rm_tokenizer(\"### Instruction:\\n테스트\\n\\n### Response:\\n좋아\", return_tensors=\"pt\")\n",
    "    mx = int(enc[\"input_ids\"].max())\n",
    "    vocab = reward_model.backbone.get_input_embeddings().num_embeddings\n",
    "    print(f\"[RM] preflight: max_id={mx}, vocab={vocab}\")\n",
    "    _ = reward_model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
    "    print(\"[RM] smoke ok\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def rm_score_texts(text_batch, max_len=512):\n",
    "    enc = rm_tokenizer(text_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device) if \"attention_mask\" in enc else None\n",
    "    return reward_model(input_ids=input_ids, attention_mask=attention_mask)  # [B]\n",
    "\n",
    "# -------------------- ДАННЫЕ --------------------\n",
    "def load_prompts_from_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        obj = json.load(f)\n",
    "    if isinstance(obj, list):\n",
    "        prompts = [ex.get(\"prompt\",\"\").strip() for ex in obj if isinstance(ex, dict) and ex.get(\"prompt\")]\n",
    "    elif isinstance(obj, dict):\n",
    "        # если вдруг dict-формат\n",
    "        arr = obj.get(\"data\") or obj.get(\"items\") or []\n",
    "        prompts = [ex.get(\"prompt\",\"\").strip() for ex in arr if isinstance(ex, dict) and ex.get(\"prompt\")]\n",
    "    else:\n",
    "        raise ValueError(\"Неподдерживаемый формат JSON\")\n",
    "    prompts = [p for p in prompts if p]\n",
    "    random.shuffle(prompts)\n",
    "    return prompts\n",
    "\n",
    "list_prompt = load_prompts_from_json(DATA_JSON)\n",
    "print(\"Загружено промптов:\", len(list_prompt))\n",
    "assert len(list_prompt) > 0\n",
    "\n",
    "# -------------------- ХЕЛПЕРЫ --------------------\n",
    "def tokenize_inputs(texts, max_len=96):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    return {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "def logprobs_from_logits(logits, ids):\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    return torch.gather(logp, -1, ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# генерация на rollout (умеренные анти-повторы)\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=8,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.15,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# -------------------- PPO LOOP --------------------\n",
    "actor.train()\n",
    "\n",
    "beta_kl   = 0.1     # усиленный KL\n",
    "kl_target = 0.1\n",
    "kl_lr     = 0.1\n",
    "adaptive_kl = True\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "UPDATES    = 200  # можно поднять дальше при ресурсе\n",
    "\n",
    "pbar = tqdm(range(UPDATES), desc=\"PPO\")\n",
    "for it in pbar:\n",
    "    # 1) батч промптов\n",
    "    s = (it * BATCH_SIZE) % max(1, len(list_prompt) - BATCH_SIZE + 1)\n",
    "    batch_raw = list_prompt[s:s+BATCH_SIZE]\n",
    "    if not batch_raw: break\n",
    "    batch_prompts = [format_prompt(p) for p in batch_raw]\n",
    "\n",
    "    # 2) rollout\n",
    "    with torch.no_grad():\n",
    "        inp = tokenize_inputs(batch_prompts, max_len=96)\n",
    "        gen_ids = actor.generate(**inp, **gen_kwargs)\n",
    "\n",
    "    # вычислим маски\n",
    "    prompt_len = inp[\"input_ids\"].shape[1]\n",
    "    attn_all   = (gen_ids != tokenizer.pad_token_id).long()\n",
    "    gen_part_ids = gen_ids[:, prompt_len:]\n",
    "    gen_mask     = (gen_part_ids != tokenizer.pad_token_id).float()  # [B, Tg]\n",
    "\n",
    "    # 3) логиты актор/якорь\n",
    "    actor_out = actor(input_ids=gen_ids, attention_mask=attn_all)\n",
    "    init_out  = initial_model(input_ids=gen_ids, attention_mask=attn_all)\n",
    "\n",
    "    # next-token shift\n",
    "    actor_logits = actor_out.logits[:, :-1, :]\n",
    "    init_logits  = init_out.logits[:,  :-1, :]\n",
    "    target_ids   = gen_ids[:,    1:]\n",
    "\n",
    "    # ограничим сгенерированную часть\n",
    "    actor_logits_gen = actor_logits[:, prompt_len-1:, :]\n",
    "    init_logits_gen  = init_logits[:,  prompt_len-1:, :]\n",
    "    target_ids_gen   = target_ids[:,   prompt_len-1:]\n",
    "\n",
    "    Tgen = gen_mask.shape[1]\n",
    "    actor_logits_gen = actor_logits_gen[:, :Tgen, :]\n",
    "    init_logits_gen  = init_logits_gen[:,  :Tgen, :]\n",
    "    target_ids_gen   = target_ids_gen[:,   :Tgen]\n",
    "\n",
    "    # 4) KL\n",
    "    logp_actor = logprobs_from_logits(actor_logits_gen, target_ids_gen)\n",
    "    logp_init  = logprobs_from_logits(init_logits_gen,  target_ids_gen)\n",
    "    per_tok_kl = (logp_actor - logp_init)\n",
    "    kl_mean = (per_tok_kl * gen_mask).sum() / gen_mask.sum().clamp(min=1)\n",
    "\n",
    "    # 5) награда от RM (на полном тексте, как RM обучался)\n",
    "    decoded_full = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        rewards_raw = rm_score_texts(decoded_full)  # [B]\n",
    "\n",
    "    # 6) центрируем и нормируем награду (стабилизация PPO)\n",
    "    rewards = (rewards_raw - rewards_raw.mean()) / (rewards_raw.std(unbiased=False) + 1e-6)\n",
    "    reward_mean = rewards.mean()\n",
    "\n",
    "    # 7) финальный лосс (actor-only surrogate)\n",
    "    loss = -(reward_mean - beta_kl * kl_mean)\n",
    "\n",
    "    actor_optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(optim_params, 1.0)\n",
    "    actor_optim.step()\n",
    "\n",
    "    # 8) adaptive KL\n",
    "    if adaptive_kl:\n",
    "        with torch.no_grad():\n",
    "            ratio = (kl_mean.item() / max(1e-8, kl_target)) - 1.0\n",
    "            beta_kl = float(max(1e-5, min(1.0, beta_kl * (1.0 + kl_lr * ratio))))\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"loss\":   f\"{loss.item():.3f}\",\n",
    "        \"reward\": f\"{reward_mean.item():.3f}\",\n",
    "        \"kl\":     f\"{kl_mean.item():.3f}\",\n",
    "        \"beta\":   f\"{beta_kl:.4f}\",\n",
    "        \"len\":    f\"{gen_ids.shape[1]}\"\n",
    "    })\n",
    "\n",
    "# -------------------- SAVE --------------------\n",
    "actor.save_pretrained(PPO_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUTPUT_DIR)\n",
    "print(\"✅ Saved PPO actor (LoRA) to:\", PPO_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MSnGYBv_ClB"
   },
   "outputs": [],
   "source": [
    "\n",
    "actor.eval()\n",
    "\n",
    "# ==== тестовые промпты ====\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?',\n",
    "    '한국에서 가장 높은 산은 어디야?',\n",
    "    '서울 지하철 2호선은 몇 시에 끊겨?',\n",
    "    'BTS 멤버 중 막내는 누구야?',\n",
    "    '코로나19 첫 발생 연도는?',\n",
    "    '한글날은 언제야?',\n",
    "    '부산에서 유명한 음식은 뭐야?',\n",
    "    '애플의 창립자는 누구야?',\n",
    "    '인공지능과 머신러닝의 차이는 뭐야?',\n",
    "    '한국의 전통 혼례에서 중요한 의식은?',\n",
    "    '세계에서 가장 긴 강은 어디야?',\n",
    "    '올해 한국 프로야구 우승팀은 누구야?',\n",
    "    '김치찌개 맛있게 끓이는 법 알려줘',\n",
    "    '삼국시대 고구려의 수도는 어디였어?',\n",
    "    '테슬라 CEO는 누구야?',\n",
    "    '아이 공부 집중력을 높이는 방법은?',\n",
    "    '우주에서 가장 가까운 별 이름은 뭐야?'\n",
    "]\n",
    "\n",
    "# ==== генерация ====\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "for prompt in list_prompt:\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(actor.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = actor.generate(**inp, **gen_kwargs)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Response:\", tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
