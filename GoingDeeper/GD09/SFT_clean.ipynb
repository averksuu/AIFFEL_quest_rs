{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxINvNpN0Sqx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "qaovqGdjz9a6"
   },
   "outputs": [],
   "source": [
    "lens_prompt=[len(x['prompt']) for x in list_data_dict]\n",
    "lens_comp=[len(x['completion']) for x in list_data_dict]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(lens_prompt)\n",
    "plt.title('length of prompt')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(lens_comp)\n",
    "plt.title('length of completion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7IzWiWm3uhr"
   },
   "source": [
    "중복된 (prompt, completion) 쌍을 제거했어요.\n",
    "\n",
    "너무 짧거나 (prompt < 5, completion < 3) / 너무 긴 데이터 (prompt > 512, completion > 1024) 를 걸러냈습니다.\n",
    "\n",
    "모든 completion 문장이 </s> 토큰으로 끝나도록 통일시켰습니다.\n",
    "\n",
    "이 과정을 통해 품질이 낮거나 모델 학습에 방해가 될 수 있는 데이터를 제거하고, 학습 안정성과 성능 향상을 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhkvpybJ1vWn"
   },
   "outputs": [],
   "source": [
    "seen=set()\n",
    "sft_clean=[]\n",
    "for x in list_data_dict:\n",
    "  prompt=x['prompt'].strip()\n",
    "  completion=x['completion'].strip()\n",
    "\n",
    "  key=(prompt, completion)\n",
    "  if key in seen:\n",
    "    continue\n",
    "  seen.add(key)\n",
    "\n",
    "  if len(prompt)<5:\n",
    "    continue\n",
    "  if len(completion)<3:\n",
    "    continue\n",
    "\n",
    "  if len(prompt)>512:\n",
    "    continue\n",
    "  if len(completion)>1024:\n",
    "    continue\n",
    "\n",
    "  if not completion.endswith('</s>'):\n",
    "    completion = completion + \"</s>\"\n",
    "\n",
    "  sft_clean.append({'prompt':prompt, 'completion':completion})\n",
    "\n",
    "out_path= \"KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.clean.jsonl\"\n",
    "with open(out_path, 'w') as f:\n",
    "  json.dump(sft_clean, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"SFT before:\", len(list_data_dict), \"after:\", len(sft_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxyxLUGy5l5c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gtU6nGDdVcI"
   },
   "source": [
    "데이터 처리 방식\n",
    "\n",
    "처음에는 DataCollatorForSupervisedDataset을 사용해서 input_ids와 labels를 따로 패딩했음.\n",
    "\n",
    "지금은 CausalLMDataCollator를 직접 정의해서 tokenizer.pad()로 한 번에 패딩하고, attention_mask==0인 부분을 -100으로 마스킹하도록 변경함.\n",
    "\n",
    "학습 설정(TrainingArguments)\n",
    "\n",
    "원래는 num_train_epochs=1, batch_size=8, warmup_steps=5, prediction_loss_only=True 등 간단한 설정.\n",
    "\n",
    "지금은 num_train_epochs=2, batch_size=2, gradient_accumulation_steps=8, warmup_steps=200, weight_decay=0.01 등 더 정교한 설정으로 바꿈.\n",
    "\n",
    "또한 logging_steps, eval_steps, save_steps, save_total_limit 등을 추가해 학습 과정을 더 세밀하게 관리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f5e457d4006e41a59f1a5590916564f6",
      "a956a38b54d14a46a675c98d71b832af",
      "35b6cf19c61f442bbd4bb34aa57516d6",
      "5c5fd69a644f4cad88717c0a7ba56c72",
      "a1a29064b9654ca9ba0cbe7808ba766d",
      "9888447824ff40c6851ef7c3fa434f25",
      "a27de44c957746dda82b7af969a034aa",
      "c802b164bc814a9494a657c39c9829bc",
      "7bf62f2d81864922a5d75e2c9710bd98",
      "27177fb116b141a4bbc537008928d436",
      "bdd7fd7ef4f043f0ad13ec5ba55b6d6c",
      "e48d2cf215e3447c87ed374e6a551a73",
      "3aa316ba88aa4f6d83981abb9fabca00",
      "d8fa5575c0ec4d38bee1fc62d66de5bd",
      "7f708ea3419c4096867b142e6d494227",
      "fa6d29bd2511405f90f2efc00ede3657",
      "4b5280d7c43d4eb5baf87a4dd9f469be",
      "b6bac282aa8743f5ad6d38be626c89f2",
      "1b73cc9de85749ecb5ab97a0637f89f5",
      "f5e42ed96ac24a8ea9a95df591826c33",
      "02466171334f435db87e19cd1df4ab67",
      "76914dbfe3c54f95b764e31e52a68db9",
      "719cde2faa43460faa40c499f4f10c7b",
      "0c5771fb924340d492ad620d79a9a5f7",
      "9284f9d98f924b64ba66c9671cb5b1f9",
      "f6ac12bda3044e41bced65f74f941d16",
      "f08e8e15f405491b91e228b7ff29aca6",
      "193d29f690aa44499c9b934a748cb3c0",
      "16c4feaa09234e9784b16aebaae9318d",
      "4dba575051764c268fad717aee22c6ba",
      "8a3a64888a8b433796e8c60348a58576",
      "6425309c5d8d434dae36aca66dcf26dd",
      "17c9a802affc4c40b54418bf87475996",
      "6c282d95d7484f16ae71eb7956e31ba9",
      "881e0cee51814c598ff51853fce360cc",
      "3cc4a80b99ca48d394a5138aafa67619",
      "2fd0176e2a8648a7bb51d04661c43d55",
      "2301751e6f774aaa8c7e25ee72ba48c7",
      "4da4162ae3964d4ab87dae1fea2ffab4",
      "b4a8188500d148b6817975778d3d978d",
      "9738c285bccd450286334416d511392e",
      "6e187d65ccc142a39c97fb0453631ca2",
      "59585bda120a4110a6c0678d3a49ad55",
      "1659323d557244f8bcffee8eaf10a584",
      "18efa1e55c5c4a3c9999c64152f7b8b4",
      "643a9cad8de04cd3b6eaa7796b91f122",
      "a312eed32c0e4b74b3918578d73a2d57",
      "8bfd1e44da924d33b2e1c502fa242ef7",
      "f9494107cdbf4305bfd20a233dabb5d5",
      "a0f1b7f8f9ca4a30b7489bd220a02c33",
      "dad94da13f904ecda78fd9e1b497975e",
      "8b9cc517b081452098ef57f179384ebd",
      "b8e9ac87bef34ba09d67957848713320",
      "a0f632e461cf4b9d82737bf42d2d0337",
      "bf97571f67f84e1f815304646149e5a0",
      "442686e7a2384f3496d7bc966da27699",
      "ea248b1bbbee4c3fadc0c9c9932896b2",
      "aaf300afba0441e1805e5b1f680fc6f6",
      "b25a5d34f8714bdebb9aa0dfa2bfc087",
      "33e315e3d0ea4929875d836e2e17e8ff",
      "93fda697b82a4e70a7b7448626a78b3f",
      "a65ed0ec448c4b39ba51dda7261a6971",
      "dc3a07088b3844c38e52a1c56ec27c1c",
      "f3fec648062043c9b337b3da0ccdf4fe",
      "a1f95397ef4147c5917dc92c47356675",
      "eddd675415e946f08591bc5bc102b1ce"
     ]
    },
    "id": "IP-AtAkD52Ly"
   },
   "outputs": [],
   "source": [
    "wsft_data=json.load(f)\n",
    "ith open('KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.clean.jsonl', 'r') as f:\n",
    "\n",
    "def format_row(r):\n",
    "    return {\"text\": f\"### Instruction:\\n{r['prompt']}\\n\\n### Response:\\n{r['completion']}\"}\n",
    "\n",
    "dataset = Dataset.from_list([format_row(r) for r in sft_data])\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "base_model=\"skt/kogpt2-base-v2\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(base_model, bos_token=\"</s>\", eos_token=\"</s>\", unk_token=\"</s>\", pad_token=\"</s>\")\n",
    "tokenizer.model_max_length=512\n",
    "tokenizer.padding_side=\"right\"\n",
    "\n",
    "def tokenize(batch):\n",
    "  return tokenizer(batch['text'], truncation=True, max_length=512)\n",
    "\n",
    "dataset_tok=dataset.map(tokenize, batched=True)\n",
    "dataset_tok=dataset_tok.remove_columns('text')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "class CausalLMDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[batch[\"attention_mask\"] == 0] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = CausalLMDataCollator(tokenizer)\n",
    "\n",
    "trainable, all_params = 0, 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable += param.numel()\n",
    "        print(\"✅ trainable:\", name, param.shape)\n",
    "\n",
    "print(f\"\\nTrainable params: {trainable:,} / {all_params:,} \"\n",
    "      f\"({100 * trainable/all_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dc3Pt8y71oC"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/kogpt2-base-v2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuQOvm_GABah"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tok[\"train\"],\n",
    "    eval_dataset=dataset_tok[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KoK_CGzPAWu6"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHI-sUVj8v8A"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMorQBW78XYQ"
   },
   "outputs": [],
   "source": [
    "save_dir = \"/content/drive/MyDrive/KoChatGPT/output_SFT_trinity345M_dynpad\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMKCsRZpWVFI"
   },
   "outputs": [],
   "source": [
    "dataset_tok['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOcad-WOdvzY"
   },
   "source": [
    "do_sample=False, num_beams=5, repetition_penalty=1.1 등 더 보수적이고 결정적인 생성으로 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw9U4eUNat8Z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "save_dir = \"/content/drive/MyDrive/KoChatGPT/output_SFT_trinity345M_dynpad\"\n",
    "base_model_id = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "model = PeftModel.from_pretrained(base_model, save_dir).eval()\n",
    "\n",
    "INSTR = \"### Instruction:\\n\"\n",
    "RESP  = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    row = dataset[\"test\"][i]\n",
    "    full_text = row[\"text\"]\n",
    "\n",
    "    assert full_text.startswith(INSTR)\n",
    "    body = full_text[len(INSTR):]\n",
    "    prompt_part, target_part = body.split(RESP, 1)\n",
    "\n",
    "    prompt_for_model = f\"{INSTR}{prompt_part}{RESP}\"\n",
    "\n",
    "    inputs = tokenizer(prompt_for_model, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            #top_p=0.9,\n",
    "            #temperature=0.7,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=1.1,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    generated_full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_resp = generated_full.replace(prompt_for_model, \"\").strip()\n",
    "\n",
    "    print(f\"\\n==== Test Example {i+1} ====\")\n",
    "    print(\"Prompt:\", prompt_part)\n",
    "    print(\"Target completion:\", target_part.strip())\n",
    "    print(\"Generated:\", generated_resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHyIhwxoWsnG"
   },
   "outputs": [],
   "source": [
    "dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA7M6j32d9Tg"
   },
   "source": [
    "이전 모델은 거의 모든 질문에 부정확하거나 엉뚱한 답변을 길게 생성했고, 사실과 맞지 않는 정보가 많았음.\n",
    "\n",
    "현재 모델은 Instruction–Response 형식을 따르며 답변이 좀 더 간결해졌지만, 여전히 “저는 인공지능 어시스턴트이기 때문에 …”라는 회피형 응답이 많음.\n",
    "\n",
    "일부 질문(예: 탈모 샴푸, 세렝게티 우기 등)에서는 어느 정도 관련 있는 내용을 포함했으나, 사실 정확도는 여전히 낮음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLdr_v58eEBQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
