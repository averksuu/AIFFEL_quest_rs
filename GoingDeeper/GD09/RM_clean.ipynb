{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OLzo3hRDo8a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_7tQLSfDo8a"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import peft  # noqa: F401\n",
    "except ImportError:\n",
    "    %pip -q install peft accelerate transformers datasets sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a9AcNXGDo8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def enable_lora(model, task_type=\"CAUSAL_LM\", r=16, alpha=32, dropout=0.05):\n",
    "    \"\"\"Навешивает LoRA-адаптеры на распространённые слои GPT-2/KoGPT2.\n",
    "    Возвращает PEFT-модель (model), готовую к обучению.\n",
    "    \"\"\"\n",
    "\n",
    "    target_modules = [\"c_attn\", \"c_proj\", \"c_fc\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=task_type,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    peft_model = get_peft_model(model, cfg)\n",
    "    try:\n",
    "        peft_model.print_trainable_parameters()\n",
    "    except Exception as e:\n",
    "        print(\"LoRA enabled (trainable params shown may be limited):\", e)\n",
    "    return peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8UHHVgrDo8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "except NameError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0cFwdXDh1eV"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLMBK-sHh2z2"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBk3Q8aCh8Du"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlruUmphh-qN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88CNKB-WgV7s"
   },
   "outputs": [],
   "source": [
    "import json, random\n",
    "from datasets import Dataset\n",
    "\n",
    "data_path = 'KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path, 'r', encoding='utf-8-sig') as f:\n",
    "    triplets = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_G1cvmfiTJF"
   },
   "outputs": [],
   "source": [
    "def triplet_to_pairs(ex):\n",
    "    pairs = []\n",
    "    comps = [ex['completion_0'], ex['completion_1'], ex['completion_2']]\n",
    "    ranks = ex['ranking']\n",
    "    idx = [0,1,2]\n",
    "    for a in range(3):\n",
    "        for b in range(a+1,3):\n",
    "            i,j = idx[a], idx[b]\n",
    "            if ranks[i] < ranks[j]:\n",
    "                chosen, rejected = comps[i], comps[j]\n",
    "            else:\n",
    "                chosen, rejected = comps[j], comps[i]\n",
    "            pairs.append({\n",
    "                \"prompt\": ex[\"prompt\"],\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected\n",
    "            })\n",
    "    return pairs\n",
    "\n",
    "pairs = []\n",
    "for ex in triplets:\n",
    "    pairs.extend(triplet_to_pairs(ex))\n",
    "\n",
    "def ok(s): return isinstance(s,str) and len(s.strip())>3\n",
    "pairs = [p for p in pairs if ok(p[\"prompt\"]) and ok(p[\"chosen\"]) and ok(p[\"rejected\"])]\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "split = int(0.9*len(pairs))\n",
    "train_data = pairs[:split]\n",
    "eval_data  = pairs[split:]\n",
    "\n",
    "print(\"train:\", len(train_data), \"eval:\", len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "138ff6429a994254bd7a41bab639c392",
      "15fc4dcfb04545139e061b25d00cd82b",
      "e107a39cd9bd46bc98569070c2b3a902",
      "4f6cbc50711942baacf4e8d682c78617",
      "ad3203c8b0ae40cf9539fd5aa4ed99e1",
      "258107ae8bf44fe4888b2fd95f0df208",
      "9ed3d5046c1b4b759f607fd7960e259c",
      "1ed8ca9c58f44082ba5f58c8b313b6cb",
      "712ab153ebe44e2f8f25d739b01b3053",
      "1540c3f910c24cadb893ece84b482db4",
      "c79b2dcfd92642ec804c76704c2dbabf",
      "27f2b7484c9a499c8960dc4921e90a27",
      "b4184d0fdd1f48ab9c9d876d2e9362ff",
      "d424ce9864e141e59701763175b05409",
      "0142774cc7214c70bc36cad09261e52a",
      "d59ff979c1fc406f8f0a9f144bd5aca9",
      "2fb0215b61264b4086cf884e27063f87",
      "ac210a42e5da4f45bf5c22903edb517f",
      "af5c1594d0cf47e1aab1510460be1675",
      "889d3c23724f41daae27766d4f633c51",
      "c710744c8d5f42f6a328e648481c76ef",
      "0dd4401403fa4060b8a346906ccd3c5f",
      "b012909506204f5c93ff42e16fe2d87c",
      "f5b38bb00e944cb3bc645007cea19781"
     ]
    },
    "id": "A-HaLqsdjU9G"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "INSTR = \"### Instruction:\\n\"\n",
    "RESP  = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "base_id = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_id,\n",
    "    bos_token=\"</s>\", eos_token=\"</s>\", unk_token=\"</s>\", pad_token=\"</s>\",\n",
    "    padding_side=\"right\", model_max_length=512\n",
    ")\n",
    "\n",
    "def format_sample(prompt, completion):\n",
    "    text = f\"{INSTR}{prompt}{RESP}{completion}{tokenizer.eos_token}\"\n",
    "    return text\n",
    "\n",
    "class RMPairsDataset:\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.data = data\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        ex = self.data[i]\n",
    "        return {\n",
    "            \"chosen_text\":   format_sample(ex[\"prompt\"], ex[\"chosen\"]),\n",
    "            \"rejected_text\": format_sample(ex[\"prompt\"], ex[\"rejected\"])\n",
    "        }\n",
    "\n",
    "train_ds = RMPairsDataset(train_data, tokenizer)\n",
    "eval_ds  = RMPairsDataset(eval_data, tokenizer)\n",
    "\n",
    "import torch\n",
    "class RMDataCollator:\n",
    "    def __init__(self, tokenizer, max_len=512):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __call__(self, batch):\n",
    "        chosen  = [b[\"chosen_text\"]   for b in batch]\n",
    "        rejected= [b[\"rejected_text\"] for b in batch]\n",
    "        enc_ch = self.tok(chosen,   return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_len)\n",
    "        enc_rj = self.tok(rejected, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            \"chosen_input_ids\": enc_ch[\"input_ids\"],\n",
    "            \"chosen_attention_mask\": enc_ch[\"attention_mask\"],\n",
    "            \"rejected_input_ids\": enc_rj[\"input_ids\"],\n",
    "            \"rejected_attention_mask\": enc_rj[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "collator = RMDataCollator(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tS8LcYFjY8K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "class GPT2RewardModel(nn.Module):\n",
    "    def __init__(self, pretrained_id=\"skt/kogpt2-base-v2\", gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.backbone = GPT2Model.from_pretrained(pretrained_id)\n",
    "        if gradient_checkpointing:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "        hidden = self.backbone.config.n_embd\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state\n",
    "        lengths = attention_mask.sum(dim=1) - 1  # [B]\n",
    "        pooled = last_hidden[torch.arange(input_ids.size(0)), lengths]  # [B,H]\n",
    "        reward = self.value_head(pooled).squeeze(-1)  # [B]\n",
    "        return reward\n",
    "\n",
    "\n",
    "def rm_step(model, batch, optimizer=None, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "    r_ch = model(batch[\"chosen_input_ids\"],   batch[\"chosen_attention_mask\"])\n",
    "    r_rj = model(batch[\"rejected_input_ids\"], batch[\"rejected_attention_mask\"])\n",
    "\n",
    "    diff = r_ch - r_rj\n",
    "    loss = -torch.log(torch.sigmoid(diff)).mean()\n",
    "    if optimizer:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        acc = (r_ch > r_rj).float().mean().item()\n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GHmyzKODo8d"
   },
   "outputs": [],
   "source": [
    "# === Включаем LoRA на только что созданной модели ===\n",
    "try:\n",
    "    model  # noqa: F821\n",
    "    _scope_model = model\n",
    "except NameError:\n",
    "    try:\n",
    "        _scope_model = base_model  # noqa: F821\n",
    "    except NameError:\n",
    "        try:\n",
    "            _scope_model = actor_model  # noqa: F821\n",
    "        except NameError:\n",
    "            try:\n",
    "                _scope_model = policy  # noqa: F821\n",
    "            except NameError:\n",
    "                try:\n",
    "                    _scope_model = rm_model  # noqa: F821\n",
    "                except NameError:\n",
    "                    try:\n",
    "                        _scope_model = backbone  # noqa: F821\n",
    "                    except NameError:\n",
    "                        _scope_model = None\n",
    "\n",
    "if _scope_model is not None:\n",
    "    # Пытаемся определить тип задачи (по классу); для GPT2 это CAUSAL_LM ок.\n",
    "    task_type = \"CAUSAL_LM\"\n",
    "    model = enable_lora(_scope_model, task_type=task_type, r=16, alpha=32, dropout=0.05)\n",
    "else:\n",
    "    print(\"Предупреждение: не удалось найти переменную модели для навешивания LoRA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "e8c3e4e7b52d400cb06ac0276a3c59ab",
      "451077e184044e7897cfe71bfcfd97f6",
      "2f93f51491e145c6a058f5e2f1c81a42",
      "8efb3520d83d42d9861a5a43361ee08c",
      "5b29606a813841c694ab30aa7811b6f1",
      "75d5ab0e00c04e26aab122585ebcb15f",
      "291644a7ce5a48999749fd8b576af0b4",
      "febab4ba8b804af48e352842e6c70404",
      "2a10bace2b94460aa315c240ee00153b",
      "d1fa3093686f49b49876d74eb1eb4dc3",
      "ec4316d2e803493a985cf8227ac5346d",
      "6b669c670fa847fd89075ea28ab94a6e",
      "eb4831a3ba684ca8a6b88f580ec01e4f",
      "1cfce8ccc60b4823b6954bcb8f096ce4",
      "ae861e002f85490eada9a01ff2c8203c",
      "dbdf82f830e544068c5853f019683c2d",
      "6d36a7fc9d274664b2baddbb873cb1be",
      "c378e79f921d41cda2ed4b355faea42f",
      "6897e8c9d31949199e956626a658d5db",
      "4cad18aa3d044ff19a01cf8eadd6fbd6",
      "8c25e00f148a4be1b7ee90cecf2aabee",
      "608b8ba554294c22b3c15f856541d84a",
      "20d9155fa02d4593a11c81c7d36d2a58",
      "143a3b0f8a824129b8b6cfe14695f3b0",
      "cb774fb0229e4739975bc9dad9c95336",
      "cdd5c4ded40b45c8b28566fac8617392",
      "3f6fafd726cd4ad5b1a734f7d2450d12",
      "f5f5bbd631bc4be4ae86e99ec0ba5a3a",
      "06ce7330ebbd4878bd2a3fa8ce07693a",
      "5e2e6f572e7247f5b505024ac51b926f",
      "e3fd0a9c2e424fb9bb53bf4777889059",
      "db9029e0ac694eba9f820441e32ae09f",
      "bd5286eeb78944f0af16f7f3fa9d13df",
      "4d13e0f157e44928a643642648bfba82",
      "53d639e0d5c847e096b7105a7b50850c",
      "737d9d3c054c4a01bb59622609446354",
      "333e902bf3494e589c7f42c7555ca3a1",
      "6a67e7c65c7644ccb4a7571ee1208c7c",
      "ccd1b133704740e59da53ccf366a9d95",
      "a148c14994114957a42aa5e85488ff72",
      "7f2749c3621643af8d9eab227a94a07b",
      "d139a330e04f406dbae664643055ac17",
      "a22ba75f65be42ebaaab80225317b55b",
      "1808e47c36fd476ca9ff975cf4564390",
      "bb8055e6843a4bfa8456a6417205e16a",
      "9762c9d49fa9460ab27b16f1aae0d98e",
      "82a0b842157047329d4e70d1701c999d",
      "076b7b360bc343c0849c1e8beea6815f",
      "54befa2800334acfb060092d7b32f4e9",
      "13b50a87c8804c7c89b43e5a08d60b63"
     ]
    },
    "id": "br--ky43jjRG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rm_model = GPT2RewardModel(pretrained_id=base_id, gradient_checkpointing=True).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(rm_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collator)\n",
    "eval_loader  = DataLoader(eval_ds,  batch_size=8, shuffle=False, collate_fn=collator)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * 2\n",
    "\n",
    "for epoch in range(2):\n",
    "    rm_model.train()\n",
    "    tot_loss = tot_acc = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=steps_per_epoch, desc=f\"Epoch {epoch+1} [train]\")\n",
    "    for step, batch in pbar:\n",
    "        loss, acc = rm_step(rm_model, batch, optimizer=opt, device=device)\n",
    "        tot_loss += loss\n",
    "        tot_acc  += acc\n",
    "\n",
    "        avg_loss = tot_loss / (step + 1)\n",
    "        avg_acc  = tot_acc / (step + 1)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.3f}\"})\n",
    "\n",
    "    rm_model.eval()\n",
    "    eval_loss = eval_acc = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(eval_loader), total=len(eval_loader), desc=f\"Epoch {epoch+1} [eval]\")\n",
    "        for step, batch in pbar:\n",
    "            loss, acc = rm_step(rm_model, batch, optimizer=None, device=device)\n",
    "            eval_loss += loss\n",
    "            eval_acc  += acc\n",
    "\n",
    "            avg_loss = eval_loss / (step + 1)\n",
    "            avg_acc  = eval_acc / (step + 1)\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.3f}\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jqiv0p8w0E8I"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SAVE_DIR = Path.cwd() / \"output_RM\"   # Или Path.home() / \"output_RM\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "rm_model.backbone.save_pretrained(SAVE_DIR)\n",
    "\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "torch.save(rm_model.value_head.state_dict(), os.path.join(SAVE_DIR, \"value_head.bin\"))\n",
    "\n",
    "print(\"✅ RM сохранена в:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wc7eZ_LwmHDN"
   },
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rm_model.eval()\n",
    "rm_model.to(device)\n",
    "\n",
    "eval_loader_pairs = DataLoader(eval_ds, batch_size=32, shuffle=False, collate_fn=RMDataCollator(tokenizer))\n",
    "\n",
    "def tensor1d(x):\n",
    "    if isinstance(x, tuple): x = x[0]\n",
    "    x = x.squeeze(-1)\n",
    "    return x\n",
    "\n",
    "tot, correct, margins = 0, 0, []\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader_pairs:\n",
    "        ch_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "        ch_ms  = batch[\"chosen_attention_mask\"].to(device)\n",
    "        rj_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "        rj_ms  = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "        r_ch = tensor1d(rm_model(ch_ids, ch_ms)).cpu().numpy()  # [B]\n",
    "        r_rj = tensor1d(rm_model(rj_ids, rj_ms)).cpu().numpy()  # [B]\n",
    "        diff = r_ch - r_rj\n",
    "        margins.extend(diff.tolist())\n",
    "        correct += (diff > 0).sum()\n",
    "        tot += diff.shape[0]\n",
    "\n",
    "pair_acc = correct / tot\n",
    "print(f\"[Pairwise] Acc={pair_acc:.3f} | mean_margin={np.mean(margins):.3f} | median_margin={np.median(margins):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBAtnAtfoeXc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_candidates(prompt, candidates, tokenizer, rm_model, max_len=512, topk=None):\n",
    "    rm_model.eval()\n",
    "    texts = [format_sample(prompt, c) for c in candidates]\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    with torch.no_grad():\n",
    "        rewards = tensor1d(rm_model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))).cpu().numpy()\n",
    "    order = np.argsort(-rewards)\n",
    "    print(f\"\\n[Prompt]\\n{prompt}\\n\")\n",
    "    for rank, idx in enumerate(order[:topk] if topk else order, 1):\n",
    "        print(f\"#{rank}  reward={rewards[idx]:.3f}\\n{candidates[idx]}\\n\")\n",
    "    return rewards, order\n",
    "\n",
    "\n",
    "my_prompt = \"서울에서 주말에 아이와 갈 만한 실내 체험 활동 추천해줘\"\n",
    "my_candidates = [\n",
    "\n",
    "    \"서울 어린이대공원 동물원이나 놀이시설도 추천해요. 입장료가 저렴해서 가족 단위 방문객이 많습니다.\",\n",
    "    \"날씨가 좋으면 잠실 롯데월드타워 전망대, 실내활동이면 국립중앙박물관 어린이박물관도 좋아요.\",\n",
    "    \"아이 연령대에 따라 다르지만, 5~7세라면 코엑스 키자니아 같은 체험형 공간이 적합합니다.\",\n",
    "\n",
    "    \"서울에는 다양한 체험 공간이 있어요. 인터넷 검색해보시면 많은 정보를 얻을 수 있습니다.\",\n",
    "    \"추천드릴 수는 없지만 서울에는 어린이들을 위한 좋은 장소가 많습니다.\",\n",
    "    \"아이와 함께라면 실내 전시관이나 놀이시설이 무난합니다.\",\n",
    "\n",
    "\n",
    "    \"저는 인공지능이기 때문에 추천이 불가능합니다.\",\n",
    "    \"알려드릴 수 없습니다.\",\n",
    "    \"모릅니다.\"\n",
    "]\n",
    "_ = score_candidates(my_prompt, my_candidates, tokenizer, rm_model, max_len=512, topk=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R37-9sPnuzyq"
   },
   "source": [
    "기존 GPTRM_custom에서 단순히 GPT2Model + value_head 구조였는데, 지금은 GPT2RewardModel과 LoRA/gradient checkpointing 같은 설정을 적용 가능하게 수정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pleIJD1Uu1iX"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "folder = \"output_RM\"\n",
    "\n",
    "shutil.make_archive(\"output_RM_backup\", 'zip', folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTMuZYBNDo8e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
