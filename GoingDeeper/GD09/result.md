# 모델 학습 및 평가 보고서

본 보고서는 주어진 예제 코드를 기반으로 SFT → RM → PPO 단계별 학습을 진행하고, 각 단계별 특징 및 한계를 분석한 결과를 정리한 문서입니다.

---

## 1. 예제 코드
프로젝트는 아래 제공된 예제 코드를 참고하여 시작했습니다.  
👉 [예제 코드 노트북 링크](https://github.com/averksuu/AIFFEL_quest_rs/blob/main/GoingDeeper/GD09/example.ipynb)

---

## 2. 1단계: SFT
첫 번째 단계로 SFT(Supervised Fine-Tuning)를 수행했습니다.  
👉 [SFT 코드 노트북 링크](https://github.com/averksuu/AIFFEL_quest_rs/blob/main/GoingDeeper/GD09/SFT_github.ipynb)

### 데이터 전처리
- **중복 제거**: 동일한 (prompt, completion) 쌍 제거  
- **길이 필터링**:  
  - 너무 짧은 데이터 제거: `prompt < 5`, `completion < 3`  
  - 너무 긴 데이터 제거: `prompt > 512`, `completion > 1024`  
- **토큰 종료 통일**: 모든 completion 문장을 토큰으로 끝나도록 변환  

→ 품질이 낮거나 불필요한 데이터를 제거함으로써 **학습 안정성과 성능 향상**을 기대할 수 있었음.

### 데이터 처리 방식
- 초기: `DataCollatorForSupervisedDataset` 사용 (input_ids와 labels를 별도 패딩)  
- 개선: `CausalLMDataCollator` 정의 → `tokenizer.pad()`로 일괄 패딩, `attention_mask==0`인 부분을 `-100`으로 마스킹  

### 학습 설정 (TrainingArguments)
- **기존 설정**: `num_train_epochs=1`, `batch_size=8`, `warmup_steps=5` 등 단순 구성  
- **개선된 설정**:  
  - `num_train_epochs=2`, `batch_size=2`, `gradient_accumulation_steps=8`  
  - `warmup_steps=200`, `weight_decay=0.01`  
  - `logging_steps`, `eval_steps`, `save_steps`, `save_total_limit` 추가 → 세밀한 학습 관리  
  - `do_sample=False`, `num_beams=5`, `repetition_penalty=1.1` 설정 → 보수적이고 결정적인 응답 생성  

### 결과
- **이전 모델**: 부정확하거나 엉뚱한 긴 답변 생성, 사실과 맞지 않는 정보 다수  
- **현재 모델**: Instruction–Response 형식을 따르며 답변이 간결해졌으나, 여전히 회피형 응답 빈번  

---

## 3. 2단계: RM (Reward Model) 학습
두 번째 단계에서는 RM(Reward Model)을 학습했습니다.  
👉 [RM 코드 노트북 링크](https://github.com/averksuu/AIFFEL_quest_rs/blob/main/GoingDeeper/GD09/RM_clean_fixed.ipynb)

### 모델 구조
- 기존: `GPTRM_custom` (단순 GPT2Model + value_head 구조)  
- 변경: `GPT2RewardModel`로 확장 → **LoRA**, **gradient checkpointing** 적용 가능  

### 결과 분석
Reward 점수를 기준으로 평가 시, 짧고 회피적인 응답을 선호하는 경향이 뚜렷하게 나타남.  

#### 예시 프롬프트
**Prompt**:  
서울에서 주말에 아이와 갈 만한 실내 체험 활동 추천해줘

- `reward=1.899` → 저는 인공지능이기 때문에 추천이 불가능합니다.  
- `reward=0.310` → 아이 연령대에 따라 다르지만, 코엑스 키자니아 같은 체험형 공간이 적합합니다.  
- `reward=-0.466` → 날씨가 좋으면 롯데월드타워 전망대, 실내활동이면 국립중앙박물관 어린이박물관도 좋아요.  

### 한계점
- 사실적·구체적인 답변보다 **짧은 회피형 응답**에 더 높은 보상이 주어짐  
- **출력 다양성 부족**, 실제 유용한 응답이 저평가됨  

---

## 4. 3단계: PPO 학습
세 번째 단계에서는 PPO(Proximal Policy Optimization)를 적용했습니다.  

👉 [PPO (RM + LoRA) 코드 노트북 링크](https://github.com/averksuu/AIFFEL_quest_rs/blob/main/GoingDeeper/GD09/PPO_1_github.ipynb)  
👉 [PPO (RM without LoRA) 코드 노트북 링크](https://github.com/averksuu/AIFFEL_quest_rs/blob/main/GoingDeeper/GD09/PPO_2_github.ipynb)

### 주요 변경 사항
1. **LoRA 적용**  
   - Actor 모델에 LoRA 어댑터를 불러와 적용  
   - 학습 시 LoRA 레이어만 `requires_grad=True`로 설정 → 경량 학습 가능  

2. **Tokenizer 설정 수정**  
   - `padding_side="left"`로 변경 (decoder-only 모델 특성 반영)  
   - PAD 토큰을 EOS 토큰으로 지정  

3. **Reward Model 로딩 방식 변경**  
   - RM을 backbone + value_head로 분리 로딩  
   - `resize_token_embeddings()` 적용하여 충돌 방지  

4. **PPO 루프 직접 구현**  
   - `logprobs_from_logits`, KL 보정, reward 계산, advantage 정규화 등을 직접 작성  
   - KL 보정은 **adaptive KL**로 동적 조정  

### 결과
- **RM + LoRA 기반 PPO**: 경량 학습 가능하나 회피적 응답 성향 지속  
- **RM without LoRA 기반 PPO**: 일부 구체적 응답 생성, 그러나 정확도 개선은 제한적  

---

## 5. 모델 비교 및 결론

### 예제 모델
- 허구적 이름·날짜·기호를 포함한 긴 답변 생성  
- 비논리적이고 현실과 동떨어짐  

### LoRA 모델
- 질문 반복 또는 “제공할 수 없습니다”와 같은 정형화된 응답  
- 자연스럽지만 유용한 정보 부족  

### Without LoRA 모델
- “웹사이트를 확인하세요” 등 회피적 표현 사용  
- 일부 응답은 사실에 근접 (예: 코로나19 발생 연도)  
- 가장 현실적이지만 여전히 한계 존재  

📌 **최종 결론**  
- 예제 모델: **허구적, 신뢰성 부족**  
- LoRA 모델: **자연스러우나 무정보적**  
- Without LoRA 모델: **상대적으로 현실적이나 정확성 미흡**  

→ 향후 개선을 위해 **보상 함수 설계 개선, 데이터 품질 강화, 인간 피드백(HF) 기반 학습**이 필수적임.

---

## 6. 모델별 응답 비교 표

| 번호 | 질문 (요약)      | `예제` – 현실성/자연스러움                         | `LoRA` – 현실성/자연스러움                      | `Without LoRA` – 현실성/자연스러움               |
| -- | ------------ | -------------------------------------------- | -------------------------------------- | ------------------------------------ |
| 1  | 불고기와 한우?     | 연결되지 않은 텍스트, 많은 대시와 임의의 기호.                | 고기를 구입할 수 없다고 답함 – 자연스럽지만 질문과는 관련이 없음. | “식당에 문의하라”는 회피 – 그럴듯하지만 도움이 되지 않음.   |
| 2  | 닉슨 부통령 연도?   | 허구의 이름과 2021년 날짜를 나열 – 사실처럼 보이지 않음.        | 1945년을 언급 – 사실처럼 들리지만 틀림.              | ‘1952–1953’이라고 하여 실제 날짜처럼 보이나 틀림.    |
| 3  | 오헤어 공항 위치?   | 일본과 멕시코를 섞어 터무니없음.                         | 표를 구매할 수 없다고 답함 – 그럴듯하지만 질문에 답하지 않음.   | 샌디에이고라고 확신에 차 답하지만 사실이 아님.           |
| 4  | 오늘 미세먼지?     | 이해할 수 없는 문자들의 긴 흐름 – 부자연스러움.               | ‘예보를 제공할 수 없다’라고 거절 – 솔직해 보임.          | ‘모른다’는 거절과 일반적인 조언 – 자연스러움.          |
| 5  | 한국 최고봉?      | ‘수원 5.18km’라 하고 무관한 기호를 덧붙임.               | 답변을 거부 – 모델로서 자연스러움.                   | 다른 곳으로 안내하며 거절 – 그럴듯함.               |
| 6  | 2호선 막차?      | 1호선 이야기를 지어내고 긴 문장 – 부자연스러움.               | 모른다고 거절 – 자연스럽지만 본질적인 답이 없음.           | 거절 – 자연스럽지만 정보 없음.                   |
| 7  | BTS 막내?      | ‘NTO’를 언급하고 반복 – 사실 같지 않음.                 | ‘모른다’라고 솔직히 답함.                        | 거절 – 자연스럽지만 정보 없음.                   |
| 8  | 코로나 첫 발생?    | 2015년과 임의의 단어를 써서 그럴듯하지 않음.                | 비논리적 텍스트 – 부자연스러움.                     | 2019년과 권장 사항을 답해 정확하고 자연스러움.         |
| 9  | 한글날 날짜?      | 날짜와 이해할 수 없는 단어들의 긴 나열 – 부자연스러움.           | 거절 – 자연스러움.                            | 1월 1일을 반복 – 실수처럼 보이지만 문장은 이해 가능.     |
| 10 | 부산 음식?       | 연관 없는 이름을 나열하고 중국어 문자를 넣어 이상함.             | 뷔페에 대한 답변 – 질문과 맞지 않음.                 | 거절 – 자연스럽지만 정보 없음.                   |
| 11 | 애플 창립자?      | Apple과 Microsoft를 혼동하고 긴 텍스트 – 그럴듯하지 않음.   | 모른다고 말함 – 자연스러움.                       | 더 명확한 설명을 요청 – 형식적인 답변.              |
| 12 | AI와 ML 차이?   | 답변이 비논리적이고 철학적 – 이해하기 어려움.                 | 데이터 수집 모델이라 말해 주제와 맞지 않음.              | 출처를 찾아보라고 권하며 거절 – 그럴듯하지만 답 없음.      |
| 13 | 한국 전통 혼례 의식? | 한복 얘기만 하고 이후는 무의미한 말 – 부자연스러움.             | ‘혼례상’을 언급하지만 의식을 설명하지 않아 불완전.          | 거절 – 솔직하지만 내용 없음.                    |
| 14 | 세계에서 가장 긴 강? | 노르웨이의 가상의 강과 시를 언급 – 터무니없음.                | 거절 – 자연스러움.                            | 거절 – 그럴듯하지만 정보 없음.                   |
| 15 | 올해 KBO 우승?   | 두 팀과 임의의 기호를 언급 – 사실 아님.                   | 롯데 자이언츠를 꼽음 – 확신에 차 보이나 사실 아님.         | 거절 – 그럴듯함.                           |
| 16 | 김치찌개 끓이는 법?  | 김치 두께와 주의사항에 대한 이상한 조언 – 레시피 같지 않음.        | 거절 – 자연스러움.                            | 거절 – 형식적이지만 레시피 없음.                  |
| 17 | 고구려 수도?      | 1590년을 언급하며 정보 없다고 함 – 그럴듯하지 않음.           | ‘5세기’라고 답함 – 너무 일반적이지만 사실처럼 보임.        | ‘기원전 600년부터 600년까지’라고 함 – 무의미함.      |
| 18 | 테슬라 CEO?     | 감정과 법에 대한 추상적인 텍스트 – 관련 없음.                | 토마스 제퍼슨 이야기를 함 – 터무니없음.                | 회사의 설명을 요청하고 형식적으로 답함 – 자연스럽지만 답 없음. |
| 19 | 아이 집중력?      | ‘논리적 기술’ 목록 – 일반적이고 주제와 맞지 않음.             | 거절 – 자연스러움.                            | 거절과 일반적인 표현 – 그럴듯하지만 가르치지 않음.        |
| 20 | 가장 가까운 별?    | ‘타코라’라는 이름과 이상한 텍스트를 만들어냄 – 사실이 아님.        | 거절 – 자연스러움.                            | 거절 – 형식적이지만 답 없음.                    |

---
📌 **표 요약**  
- `예제` 모델: **허구적 정보 많음, 비논리적**  
- `LoRA` 모델: **자연스럽지만 정보 부족**  
- `Without LoRA` 모델: **가장 현실적이나 여전히 정확성 한계**
