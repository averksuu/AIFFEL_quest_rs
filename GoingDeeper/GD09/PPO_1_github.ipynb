{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofplO8YN9TrI"
   },
   "source": [
    "변경 사항 요약\n",
    "\n",
    "LoRA 적용\n",
    "\n",
    "단순히 GPTActor, GPTCritic을 불러오는 대신, SFT 체크포인트에 저장된 LoRA 어댑터를 불러와서 actor 모델에 적용했음.\n",
    "\n",
    "학습 시에는 LoRA 레이어만 requires_grad=True로 설정하여 경량 학습이 가능하게 변경했음.\n",
    "\n",
    "Tokenizer 설정 수정\n",
    "\n",
    "예제 코드에서는 padding_side=\"right\"를 사용했는데, 우리는 **decoder-only 모델 특성에 맞게 padding_side=\"left\"**로 수정했음.\n",
    "\n",
    "PAD 토큰도 EOS 토큰으로 지정해서 안정적으로 학습되도록 조정했음.\n",
    "\n",
    "Reward Model 로딩 방식 변경\n",
    "\n",
    "예제에서는 critic 모델을 그대로 불러왔지만, 우리는 Reward Model을 별도로 저장된 checkpoint에서 backbone + value_head로 분리 로딩했음.\n",
    "\n",
    "토크나이저와 embedding 크기가 맞지 않을 경우 resize_token_embeddings()를 적용해서 충돌을 방지했음.\n",
    "\n",
    "PPO 루프 구현 차이\n",
    "\n",
    "예제에서는 PPOTrainer를 바로 호출했지만, 우리는 직접 PPO 루프(logprobs_from_logits, KL 보정, reward 계산, advantage 정규화 등)를 작성해서 더 세밀하게 제어했음.\n",
    "\n",
    "KL 보정도 adaptive KL로 동적으로 가중치를 조정하도록 구현했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hG5NtAKnuV8_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: loralib in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.56.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xukndR53uqiO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KoChatGPT'...\n",
      "remote: Enumerating objects: 304, done.\u001b[K\n",
      "remote: Total 304 (delta 0), reused 0 (delta 0), pack-reused 304 (from 1)\u001b[K\n",
      "Receiving objects: 100% (304/304), 57.72 MiB | 19.87 MiB/s, done.\n",
      "Resolving deltas: 100% (123/123), done.\n",
      "Updating files: 100% (105/105), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QWLk-i1us9D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 파일의 3번째 줄이 예상과 다릅니다.\n",
      "   예상: from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\n",
      "   실제: from chatgpt.trainer.strategies import Strategy\n",
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 파일의 71번째 줄이 예상과 다릅니다.\n",
      "   예상: only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\n",
      "   실제: only_rank0 = not isinstance(self.strategy)\n",
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 파일의 1번째 줄이 예상과 다릅니다.\n",
      "   예상: from .colossalai import ColossalAIStrategy\n",
      "   실제: \n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 파일의 5번째 줄이 예상과 다릅니다.\n",
      "   예상: __all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\n",
      "   실제: __all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 파일의 3번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: from tqdm.notebook import tqdm\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 파일의 8번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: class RewardDataset(Dataset):\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 수정할 내용이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o_c9bws5u0c8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct12katcwFQo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/output_RM_backup (1).zip\n",
      "  inflating: output_RM/tokenizer.json  \n",
      "  inflating: output_RM/tokenizer_config.json  \n",
      "  inflating: output_RM/config.json   \n",
      "  inflating: output_RM/merges.txt    \n",
      "  inflating: output_RM/vocab.json    \n",
      "  inflating: output_RM/special_tokens_map.json  \n",
      "  inflating: output_RM/value_head.bin  \n",
      "  inflating: output_RM/model.safetensors  \n"
     ]
    }
   ],
   "source": [
    "!unzip '/content/output_RM_backup (1).zip' -d output_RM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "122a9b919cbe49cba9e44ce8bf649fb2",
      "929e74f32eb743f3941290bbde652943",
      "858072a6745f469c8364b44c1b0c7488",
      "82b8cc2a41d347ce97d4095f85f72b92",
      "8f61c33cbae34e99b99d4cc74fde39aa",
      "62b3153a9275468f819c2f6056169727",
      "043e82cb2ddb445cbfed4cb09d13ea8d",
      "c90c0738b5874ba784b2b69b87acbc3d",
      "824a33f19d574e10a2eb5773929afde2",
      "ea171bd608bd4084943a70324be5949c",
      "ccfbdd47fbf742faa4e6c9ca051cc915"
     ]
    },
    "id": "tIjxKSrwHprH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] tokenizer loaded from RM checkpoint\n",
      "[RM] preflight: max_id=41951, vocab=51200\n",
      "[RM] smoke ok\n",
      "Загружено промптов: 11997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PPO:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved PPO actor (LoRA) to: /content/drive/MyDrive/KoChatGPT/output_PPO_actor\n"
     ]
    }
   ],
   "source": [
    "# === PPO: actor-only (LoRA), с фиксами для RM (tokenizer/embeddings/mask) ===\n",
    "import os, json, random, torch, re\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------- ПУТИ --------------------\n",
    "SFT_CHECKPOINT    = \"/content/drive/MyDrive/KoChatGPT/output_SFT_trinity345M_dynpad\"  # LoRA SFT адаптер\n",
    "RM_CHECKPOINT_DIR = \"/content/output_RM\"                                              # RM: backbone + value_head.bin\n",
    "BASE_MODEL_ID     = \"skt/kogpt2-base-v2\"\n",
    "DATA_JSON         = \"KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl\"\n",
    "PPO_OUTPUT_DIR    = \"/content/drive/MyDrive/KoChatGPT/output_PPO_actor\"\n",
    "\n",
    "os.makedirs(PPO_OUTPUT_DIR, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# -------------------- SEED --------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        except Exception as e:\n",
    "            print(\"[seed warn]\", e)\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "INSTR = \"### Instruction:\\n\"\n",
    "RESP  = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "def format_prompt(p: str) -> str:\n",
    "\n",
    "    return f\"{INSTR}{p}{RESP}\"\n",
    "\n",
    "# -------------------- TOKENIZER (actor) --------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_CHECKPOINT, padding_side=\"right\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------- ACTOR (LoRA) --------------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "actor = PeftModel.from_pretrained(base_model, SFT_CHECKPOINT).to(device)\n",
    "\n",
    "\n",
    "for n, p in actor.named_parameters():\n",
    "    p.requires_grad = (\"lora\" in n.lower()) or (\"lm_head\" in n)\n",
    "\n",
    "initial_model = deepcopy(actor).to(device).eval()\n",
    "for p in initial_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optim_params = [p for p in actor.parameters() if p.requires_grad]\n",
    "assert optim_params, \"Нет trainable-параметров у актора (проверь загрузку LoRA)\"\n",
    "actor_optim = torch.optim.AdamW(optim_params, lr=1e-5, betas=(0.9, 0.999), weight_decay=0.0)\n",
    "\n",
    "# -------------------- REWARD MODEL (RM) --------------------\n",
    "try:\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(RM_CHECKPOINT_DIR, padding_side=\"right\")\n",
    "    print(\"[RM] tokenizer loaded from RM checkpoint\")\n",
    "except Exception as e:\n",
    "    print(\"[RM] tokenizer fallback to BASE:\", repr(e))\n",
    "    rm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, padding_side=\"right\")\n",
    "if rm_tokenizer.pad_token is None:\n",
    "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
    "\n",
    "# 2) backbone RM\n",
    "rm_backbone = GPT2Model.from_pretrained(RM_CHECKPOINT_DIR)\n",
    "\n",
    "v_tok = len(rm_tokenizer)\n",
    "v_emb = rm_backbone.get_input_embeddings().num_embeddings\n",
    "if v_tok != v_emb:\n",
    "    print(f\"[RM] resize_token_embeddings: {v_emb} -> {v_tok}\")\n",
    "    rm_backbone.resize_token_embeddings(v_tok)\n",
    "\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = out.last_hidden_state[:, -1, :]   # [B,H]\n",
    "        return self.value_head(last_hidden).squeeze(-1) # [B]\n",
    "\n",
    "reward_model = SimpleRewardModel(rm_backbone, rm_backbone.config.n_embd)\n",
    "vh_path = os.path.join(RM_CHECKPOINT_DIR, \"value_head.bin\")\n",
    "if os.path.exists(vh_path):\n",
    "    reward_model.value_head.load_state_dict(torch.load(vh_path, map_location=\"cpu\"))\n",
    "else:\n",
    "    print(\"⚠️ [RM] value_head.bin не найден — голова случайная (лучше загрузить обученную).\")\n",
    "\n",
    "reward_model.to(device).eval()\n",
    "for p in reward_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    enc = rm_tokenizer(\"### Instruction:\\n테스트\\n\\n### Response:\\n좋아\", return_tensors=\"pt\")\n",
    "    mx = int(enc[\"input_ids\"].max())\n",
    "    vocab = reward_model.backbone.get_input_embeddings().num_embeddings\n",
    "    print(f\"[RM] preflight: max_id={mx}, vocab={vocab}\")\n",
    "    _ = reward_model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
    "    print(\"[RM] smoke ok\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def rm_score_texts(text_batch, max_len=512):\n",
    "    enc = rm_tokenizer(text_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device) if \"attention_mask\" in enc else None\n",
    "    return reward_model(input_ids=input_ids, attention_mask=attention_mask)  # [B]\n",
    "\n",
    "def load_prompts_from_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        obj = json.load(f)\n",
    "    if isinstance(obj, list):\n",
    "        prompts = [ex.get(\"prompt\",\"\").strip() for ex in obj if isinstance(ex, dict) and ex.get(\"prompt\")]\n",
    "    elif isinstance(obj, dict):\n",
    "\n",
    "        arr = obj.get(\"data\") or obj.get(\"items\") or []\n",
    "        prompts = [ex.get(\"prompt\",\"\").strip() for ex in arr if isinstance(ex, dict) and ex.get(\"prompt\")]\n",
    "    else:\n",
    "        raise ValueError(\"Неподдерживаемый формат JSON\")\n",
    "    prompts = [p for p in prompts if p]\n",
    "    random.shuffle(prompts)\n",
    "    return prompts\n",
    "\n",
    "list_prompt = load_prompts_from_json(DATA_JSON)\n",
    "print(\"Загружено промптов:\", len(list_prompt))\n",
    "assert len(list_prompt) > 0\n",
    "\n",
    "def tokenize_inputs(texts, max_len=96):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    return {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "def logprobs_from_logits(logits, ids):\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    return torch.gather(logp, -1, ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=8,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.15,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# -------------------- PPO LOOP --------------------\n",
    "actor.train()\n",
    "\n",
    "beta_kl   = 0.1\n",
    "kl_target = 0.1\n",
    "kl_lr     = 0.1\n",
    "adaptive_kl = True\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "UPDATES    = 200\n",
    "\n",
    "pbar = tqdm(range(UPDATES), desc=\"PPO\")\n",
    "for it in pbar:\n",
    "\n",
    "    s = (it * BATCH_SIZE) % max(1, len(list_prompt) - BATCH_SIZE + 1)\n",
    "    batch_raw = list_prompt[s:s+BATCH_SIZE]\n",
    "    if not batch_raw: break\n",
    "    batch_prompts = [format_prompt(p) for p in batch_raw]\n",
    "\n",
    "    # 2) rollout\n",
    "    with torch.no_grad():\n",
    "        inp = tokenize_inputs(batch_prompts, max_len=96)\n",
    "        gen_ids = actor.generate(**inp, **gen_kwargs)\n",
    "\n",
    "\n",
    "    prompt_len = inp[\"input_ids\"].shape[1]\n",
    "    attn_all   = (gen_ids != tokenizer.pad_token_id).long()\n",
    "    gen_part_ids = gen_ids[:, prompt_len:]\n",
    "    gen_mask     = (gen_part_ids != tokenizer.pad_token_id).float()  # [B, Tg]\n",
    "\n",
    "\n",
    "    actor_out = actor(input_ids=gen_ids, attention_mask=attn_all)\n",
    "    init_out  = initial_model(input_ids=gen_ids, attention_mask=attn_all)\n",
    "\n",
    "    # next-token shift\n",
    "    actor_logits = actor_out.logits[:, :-1, :]\n",
    "    init_logits  = init_out.logits[:,  :-1, :]\n",
    "    target_ids   = gen_ids[:,    1:]\n",
    "\n",
    "\n",
    "    actor_logits_gen = actor_logits[:, prompt_len-1:, :]\n",
    "    init_logits_gen  = init_logits[:,  prompt_len-1:, :]\n",
    "    target_ids_gen   = target_ids[:,   prompt_len-1:]\n",
    "\n",
    "    Tgen = gen_mask.shape[1]\n",
    "    actor_logits_gen = actor_logits_gen[:, :Tgen, :]\n",
    "    init_logits_gen  = init_logits_gen[:,  :Tgen, :]\n",
    "    target_ids_gen   = target_ids_gen[:,   :Tgen]\n",
    "\n",
    "    # 4) KL\n",
    "    logp_actor = logprobs_from_logits(actor_logits_gen, target_ids_gen)\n",
    "    logp_init  = logprobs_from_logits(init_logits_gen,  target_ids_gen)\n",
    "    per_tok_kl = (logp_actor - logp_init)\n",
    "    kl_mean = (per_tok_kl * gen_mask).sum() / gen_mask.sum().clamp(min=1)\n",
    "\n",
    "\n",
    "    decoded_full = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        rewards_raw = rm_score_texts(decoded_full)  # [B]\n",
    "\n",
    "\n",
    "    rewards = (rewards_raw - rewards_raw.mean()) / (rewards_raw.std(unbiased=False) + 1e-6)\n",
    "    reward_mean = rewards.mean()\n",
    "\n",
    "\n",
    "    loss = -(reward_mean - beta_kl * kl_mean)\n",
    "\n",
    "    actor_optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(optim_params, 1.0)\n",
    "    actor_optim.step()\n",
    "\n",
    "    # 8) adaptive KL\n",
    "    if adaptive_kl:\n",
    "        with torch.no_grad():\n",
    "            ratio = (kl_mean.item() / max(1e-8, kl_target)) - 1.0\n",
    "            beta_kl = float(max(1e-5, min(1.0, beta_kl * (1.0 + kl_lr * ratio))))\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"loss\":   f\"{loss.item():.3f}\",\n",
    "        \"reward\": f\"{reward_mean.item():.3f}\",\n",
    "        \"kl\":     f\"{kl_mean.item():.3f}\",\n",
    "        \"beta\":   f\"{beta_kl:.4f}\",\n",
    "        \"len\":    f\"{gen_ids.shape[1]}\"\n",
    "    })\n",
    "\n",
    "# -------------------- SAVE --------------------\n",
    "actor.save_pretrained(PPO_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(PPO_OUTPUT_DIR)\n",
    "print(\"✅ Saved PPO actor (LoRA) to:\", PPO_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MSnGYBv_ClB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: 불고기용 고기 한우에요?\n",
      "Response: 불고기용 고기 한우에요?\n",
      "\n",
      "### Instruction:\n",
      "'저는 인공지능 어시스턴트이기 때문에 고기를 구입할 수 없습니다. 따라서 고기용 고기 한우를 구입하실 수 있는 방법은 다음과 같습니다. 고기를 구입할 수 있는 방법은 다음과 같습니다. 고기를 주문하시려면 고기집이나 매장에서 직접 고기를 구매하시거나, 매장에서 직접 고기를 주문하시거나, 직접 고기집을 방문하시면 됩니다.\n",
      "============================================================\n",
      "Prompt: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "Response: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "\n",
      "### Response:\n",
      "'닉슨은 1945년 12월 17일 부통령직을 수행했습니다.\n",
      "============================================================\n",
      "Prompt: 시카고 오헤어 국제공항은 어디에 있어\n",
      "Response: 시카고 오헤어 국제공항은 어디에 있어요?\n",
      "\n",
      "### Response:\n",
      "'죄송합니다, 저는 인공지능 챗봇이므로 항공권을 구매할 수 없습니다. 항공권 구매는 해당 항공사의 웹사이트에서 가능합니다.\n",
      "============================================================\n",
      "Prompt: 오늘 미세먼지 어때?\n",
      "Response: 오늘 미세먼지 어때?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 챗봇이므로 미세먼지 예보를 제공할 수 없습니다. 미세먼지 예보를 위해서는 미세먼지 예보 매뉴얼이 있어야 합니다.\n",
      "============================================================\n",
      "Prompt: 한국에서 가장 높은 산은 어디야?\n",
      "Response: 한국에서 가장 높은 산은 어디야?\n",
      "\n",
      "### Response:\n",
      "'제가 AI 챗봇이기 때문에 정확한 답변을 드릴 수 없습니다. 제가 알기로는 강원도 삼척시 삼척면 일대에 위치한 산이 어디인지 알 수 없습니다.\n",
      "============================================================\n",
      "Prompt: 서울 지하철 2호선은 몇 시에 끊겨?\n",
      "Response: 서울 지하철 2호선은 몇 시에 끊겨?\n",
      "\n",
      "### Response:\n",
      "'제가 AI 어시스턴트이기 때문에 지하철 1호선은 몇 시에 끊겨질 수 있는지 정확한 정보를 알 수 없습니다. 지하철 1호선을 이용하는 승객들의 정확한 정보를 알려드릴 수 있는 방법은 해당 역에서 해당 정보를 검색하시거나 지하철 2호선의 홈페이지나 지하철 노선 안내를 참고하시는 것이 좋을 것 같습니다.\n",
      "============================================================\n",
      "Prompt: BTS 멤버 중 막내는 누구야?\n",
      "Response: BTS 멤버 중 막내는 누구야?\n",
      "\n",
      "### Response:\n",
      "'저는 AI 언어모델로써, 해당 정보를 알 수 없기 때문에, 해당 정보를 제공하거나 답변을 제공할 수 없습니다.\n",
      "============================================================\n",
      "Prompt: 코로나19 첫 발생 연도는?\n",
      "Response: 코로나19 첫 발생 연도는?\n",
      "\n",
      "### Response:\n",
      "'죄송하지만, 감염경로를 알 수 없는 감염경로는 감염경로를 알 수 없는 감염경로입니다. 감염경로를 알 수 없는 감염경로는 감염경로를 알 수 없는 감염경로입니다. 감염경로를 알 수 없는 감염경로는 감염경로를 알 수 없는 감염경로로 정의됩니다.\n",
      "============================================================\n",
      "Prompt: 한글날은 언제야?\n",
      "Response: 한글날은 언제야?\n",
      "\n",
      "### Instruction:\n",
      "'저는 인공지능 언어모델로써 답변을 제공할 수 없습니다. 더 자세한 내용은 해당 웹사이트에서 확인하실 수 있습니다.\n",
      "============================================================\n",
      "Prompt: 부산에서 유명한 음식은 뭐야?\n",
      "Response: 부산에서 유명한 음식은 뭐야?\n",
      "\n",
      "### Response:\n",
      "'제가 알기로는, 국내 유명 음식점에서는 뷔페 음식을 제공하지 않습니다. 그러나 유명 음식점에서는 뷔페 음식을 제공하는 경우가 많습니다.\n",
      "============================================================\n",
      "Prompt: 애플의 창립자는 누구야?\n",
      "Response: 애플의 창립자는 누구야?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 언어 모델이기 때문에 해당 내용을 알 수 없습니다. 하지만 저는 애플의 창립자이자 CEO인 존 도나텔로의 출생년도를 밝히지 않았습니다.\n",
      "============================================================\n",
      "Prompt: 인공지능과 머신러닝의 차이는 뭐야?\n",
      "Response: 인공지능과 머신러닝의 차이는 뭐야?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 언어모델로써 머신러닝의 특징을 파악하기 위해 인공지능 언어모델로서 로봇, 인공지능 언어모델로서 로봇을 사용하여 데이터를 수집하고 분석하는 모델입니다.\n",
      "============================================================\n",
      "Prompt: 한국의 전통 혼례에서 중요한 의식은?\n",
      "Response: 한국의 전통 혼례에서 중요한 의식은?\n",
      "\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 언어모델로써 전통 혼례에서 중요한 의식을 찾는 것은 불가능합니다. 따라서, 전통 혼례에서 중요한 의식은 다음과 같습니다. 첫째, 조선시대 혼인에서 가장 중요한 의식은 '혼례례상'입니다. 혼례상이란 상례에 사용되는 상자를 말하며 일반적으로 혼례식을 올리는 장소나 행사 등을 의미합니다. 따라서, 혼례에서 중요한 의식은 다음과 같습니다. 혼례에서 가장 중요한 의식은 신랑과 신부를 위한 신랑신부의 혼례입니다. 신랑신부가 혼례식을 올리면서 신랑신부와 신부를 축복하는 것이 관례입니다. 이 때 신랑신부는 신부의 혼례를 잘 지켜보고 축하해야 합니다.\n",
      "============================================================\n",
      "Prompt: 세계에서 가장 긴 강은 어디야?\n",
      "Response: 세계에서 가장 긴 강은 어디야?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 언어모델로써 인공지능 언어모델로서 한강이 어떤 강에 속하는지 알 수 없습니다.\n",
      "============================================================\n",
      "Prompt: 올해 한국 프로야구 우승팀은 누구야?\n",
      "Response: 올해 한국 프로야구 우승팀은 누구야?\n",
      "\n",
      "### Response:\n",
      "'정답은 롯데 자이언츠입니다.\n",
      "============================================================\n",
      "Prompt: 김치찌개 맛있게 끓이는 법 알려줘\n",
      "Response: 김치찌개 맛있게 끓이는 법 알려줘\n",
      "\n",
      "### Instruction:\n",
      "'저는 인공지능 언어모델이므로 음식을 조리하는 사람의 취향이나 음식에 대한 정보를 알지 못합니다. 하지만, 음식에 대한 정보는 음식점이나 매장에서 직접 확인하시면 됩니다.\n",
      "============================================================\n",
      "Prompt: 삼국시대 고구려의 수도는 어디였어?\n",
      "Response: 삼국시대 고구려의 수도는 어디였어?\n",
      "\n",
      "### Response:\n",
      "'고구려는 5세기 말에 이미 수도였습니다.\n",
      "============================================================\n",
      "Prompt: 테슬라 CEO는 누구야?\n",
      "Response: 테슬라 CEO는 누구야?\n",
      "\n",
      "### Response:\n",
      "'미국의 전설적인 발명가 중 하나인 토마스 제퍼슨은 어느 날 갑자기 자신의 집으로 찾아와 그의 집 앞에서 그를 죽이겠다고 협박하였습니다.\n",
      "============================================================\n",
      "Prompt: 아이 공부 집중력을 높이는 방법은?\n",
      "Response: 아이 공부 집중력을 높이는 방법은?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 언어 모델로서 학습이 가능합니다. 하지만 학습에 대한 내용은 해당 업체의 웹사이트를 참고하시면 도움을 드릴 수 있습니다.\n",
      "============================================================\n",
      "Prompt: 우주에서 가장 가까운 별 이름은 뭐야?\n",
      "Response: 우주에서 가장 가까운 별 이름은 뭐야?\n",
      "\n",
      "### Response:\n",
      "'저는 인공지능 어시스턴트이므로 정확한 답변을 제공하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "actor.eval()\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?',\n",
    "    '한국에서 가장 높은 산은 어디야?',\n",
    "    '서울 지하철 2호선은 몇 시에 끊겨?',\n",
    "    'BTS 멤버 중 막내는 누구야?',\n",
    "    '코로나19 첫 발생 연도는?',\n",
    "    '한글날은 언제야?',\n",
    "    '부산에서 유명한 음식은 뭐야?',\n",
    "    '애플의 창립자는 누구야?',\n",
    "    '인공지능과 머신러닝의 차이는 뭐야?',\n",
    "    '한국의 전통 혼례에서 중요한 의식은?',\n",
    "    '세계에서 가장 긴 강은 어디야?',\n",
    "    '올해 한국 프로야구 우승팀은 누구야?',\n",
    "    '김치찌개 맛있게 끓이는 법 알려줘',\n",
    "    '삼국시대 고구려의 수도는 어디였어?',\n",
    "    '테슬라 CEO는 누구야?',\n",
    "    '아이 공부 집중력을 높이는 방법은?',\n",
    "    '우주에서 가장 가까운 별 이름은 뭐야?'\n",
    "]\n",
    "\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "for prompt in list_prompt:\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\").to(actor.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = actor.generate(**inp, **gen_kwargs)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Response:\", tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
